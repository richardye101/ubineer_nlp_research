{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maimport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstd_func\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2294\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2292\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2294\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/IPython/extensions/autoreload.py:599\u001b[0m, in \u001b[0;36mAutoreloadMagics.aimport\u001b[0;34m(self, parameter_s, stream)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _module \u001b[38;5;129;01min\u001b[39;00m [_\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m modname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)]:\n\u001b[0;32m--> 599\u001b[0m         top_module, top_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;66;03m# Inject module to user namespace\u001b[39;00m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39mpush({top_name: top_module})\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/IPython/extensions/autoreload.py:187\u001b[0m, in \u001b[0;36mModuleReloader.aimport_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"Import a module, and mark it reloadable\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03mReturns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmark_module_reloadable(module_name)\n\u001b[0;32m--> 187\u001b[0m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m top_name \u001b[38;5;241m=\u001b[39m module_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    189\u001b[0m top_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[top_name]\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/ubineer_nlp_research/content/mary/../std_func.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsetree, singularize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pattern'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport std_func\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Topic Modelling\n",
    "Change is important for businesses to hold a competitive edge and to meet the ever-changing needs of customers. Netflix and General Electric are two examples of companies that have evolved to adapt to the fast-moving trends of their respective industry. On the other hand, businesses like Blockbuster and MySpace failed to innovate and adapt to the moving trends. Investors often spend a great deal of time reading through financial reports to detect signs of change and adaption, so our work systematically summarises the business model of a company over time. \n",
    "\n",
    "In this section, we apply several topic modeling techniques to the Business description of filings between the years 2016 and 2018 to detect differences and emerging themes within a company. By finding these differences, we can see how the company has evolved over the years and understand shifts in the operation of the company. More specifically, we explored Non-Negative Matrix Factorization (NMF), Latent Dirichlet Allocation (LDA), and Latent Semantic Analysis (LSA) for topic modeling.\n",
    "\n",
    "We use Netflix (NTFL) and General Electric (GE) as Proof of Concept to test our topic models since they are companies that have evolved drastically over the past 15 years.\n",
    "\n",
    "##### Netflix\n",
    "Netflix was founded by Reed Hastings and Marc Rudolph in 1997 as a DVD rental-by-mail business. A year later, Netflix introduced a subscription model where customers could rent DVDs online for a fixed fee per month. In 2007, it entered the market of video streaming where anyone could enjoy live streaming videos on their computer for a monthly subscription fee. Around this time, the world was getting accustomed to the internet and technology was advancing rapidly. In 2009, the company began partnering with electronic companies to get Netflix on multiple devices like smart TVs and gaming consoles. This move attracted audiences with different background profiles and pushed Netflix to the top of the video-streaming industry. In 2011, Netflix introduced its mobile apps and ios service for smartphone users.\n",
    "\n",
    "Throughout the years, Netflix stayed competitive by changing its business strategy with the advancement of technology and catering to changing customer needs.\n",
    "\n",
    "\n",
    "##### General Electric\n",
    "General Electric (GE) was founded in 1892 and currently operates through eight industrial segments: Aviation, Healthcare, Transportation, Renewable Energy, Oil & Gas, Appliances & Lighting, Power & Water, and Capital. GE Aviation is GE's most profitable division. It made steps forward in recent years, namely in 2007 by acquiring Smith Aerospace, an American aircraft engine, and aircraft parts manufacturer, and in 2012 by acquiring Avio S.p.A., an Italy-based manufacturer of aviation propulsion components and systems for civil and military aircraft. On the other hand, GE Healthcare had slow growth in the years 2010 to 2015 but saw a significant increase in profits in 2016 by more than 0.3 billion dollars compared to the year before. GE Power & Water, GE Renewable Energy, and GE Oil & Gas were all under GE Energy up until it split in 2012. In 2014, GE Power made moves to purchase French gas turbine company Alstom for $\\$$ 13 billion dollars. Unfortunately, this move coincided with a global downturn in the price of renewables, lessening the demand for the gas turbines, and did not bring the profits that GE had hoped for. 2014 was also the year that GE agreed to sell GE Appliances to Electrolux, a Swedish appliance manufacturer and the second-largest consumer appliance manufacturer after Whirlpool Corporation, for $\\$$3.3 billion in cash.\n",
    "\n",
    "In the history of the business, GE suffered through the financial crisis in 2008 and began its downfall. From 2008 to 2017, the company consistently slashed its dividends year over year and laid off thousands of employees across all divisions. However, in 2018, GE made significant improvements in cutting debt and raising capital by selling off subsidiaries. In 2021, GE decided to separate GE HealthCare and GE Power into public companies and focus mainly on GE Aviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Dynamic Topic Modelling with Netflix, GE\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m targetComp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/dynamic_companies.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m netflix \u001b[38;5;241m=\u001b[39m targetComp[targetComp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinancialEntity\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinancialEntities/params;cik=1065280\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m ge \u001b[38;5;241m=\u001b[39m targetComp[targetComp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinancialEntity\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinancialEntities/params;cik=40545\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "## Dynamic Topic Modelling with Netflix, GE\n",
    "targetComp = pd.read_csv(\"../data/dynamic_companies.csv\")\n",
    "netflix = targetComp[targetComp[\"financialEntity\"] == \"financialEntities/params;cik=1065280\"].sort_values([\"reportingDate\"])\n",
    "ge = targetComp[targetComp[\"financialEntity\"] == \"financialEntities/params;cik=40545\"].sort_values([\"reportingDate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-Negative Matrix Factorization\n",
    "Non-negative matrix factorization uses linear algebra to discover underlying relationships between texts. It factorizes/decomposes high-dimensionality vectors(ie. TF-IDF or BOW embeddings) into a lower-dimensional representation. Given an original matrix obtained using TF-IDF or any word embedding algorithm of size MxN where M is the number of documents and N is the number of n-grams, NMF generates the **Feature** matrix and **Components** matrix. The Features matrix represents the weights of topics for each document and the Component matrix represents the weights of words for each topic. NMF modifies the values of the initial Feature matrix and Components matrix so that the product approaches the original matrix until approximation error converges or max iterations are reached (ie. $Original Matrix \\approx$ $Features \\times Components$). The matrices generated by NMF will only give non-negative values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF is very sensitive to the hyperparameters such as the number of topics, so we can use coherence scores to evaluate the most optimal number of topics so that each topic is human interpretable. The coherence of a topic, used as a proxy for topic quality, is based on the distributional hypothesis that states that words with similar meanings tend to co-occur within a similar context.\n",
    "\n",
    "A coherence score measures the relative distance between words within a topic. There are several coherence metrics but the most popular one is CV, which is the metric that we will use in our report. CV coherence score creates content vectors of words using their co-occurrences (ie. co-occurrence of \"Las\" and \"Vegas\" would be very high) and calculates the score using normalized pointwise mutual information (NPMI) and cosine similarity. CV coherence score is based on three parts: (i) calculation of word or word pair probabilities (i.e. $P(w_i)$ or $P(w_i, w_j)$ (ii) calculation of a confirmation measure using NPMI (iii) aggregation of individual confirmation measures into an overall coherence score.\n",
    "\n",
    "(i)Probabilities of single words  $P(w_i)$ or the joint probabilizy of two words $P(w_i, w_j)$ can be estimated by Boolean document calculation, that is, the number of documents in which $w_i$ or $(w_i, w_j)$ occurs, divided by the total number of documents.\n",
    "\n",
    "(ii) The confirmation measure is calculated by using NPMI, which is the likelihood of the co-occurrence of two words, taking into account the fact that it might be caused by the frequency of the single words. \n",
    "\n",
    "(1) $NPMI(w_i, w_j) = \\frac{log\\frac{P(w_i, w_j)+\\epsilon}{P(w_i)P(w_j)}}{-log(P(w_i, w_j)+\\epsilon)}$\n",
    "\n",
    "(2) $\\vec{v} = {\\sum_{w_i, w_j\\in words} {NPMI(w_i, w_j)}} $\n",
    " \n",
    "(3) $\\phi_{s_i}(\\vec{u}, \\vec{w})  = \\frac{ {\\sum_{i=1}^{|words|}u_i \\cdot w_i}}{{\\lVert \\vec{u} \\rVert}_2 \\cdot{\\lVert \\vec{w} \\rVert}_2}$\n",
    "\n",
    "(iii) We calculate the global coherence of the topic as the arithmetic mean of all confirmation measures $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the metric over a range of 3 to 40 topics, incrementing by 3, and achieved the result below. Although coherence is highest for 6 topics, we know that there are close to 50 different categories of companies in the dataset and thus 6 topics will not give well-separated results. Therefore, we chose the give our model 18 topics, which has the next highest coherence score, and will not give topics that are too specific to this set of data. If you're interested in the code, see this  [file](https://richardye101.github.io/ubineer_nlp_research/lab?path=mary%2FWeek+10+-+NMF+vs+LDA+vs+LSA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NMF Coherence](../images/nmf_coherence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below illustrates the results produced by the NMF model tuned to generate 18 topics. Each column is a topic identified by the column index and is represented by the top 10 words in the topic by weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NMF\n\u001b[0;32m----> 3\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered_timeseries_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m filtered\u001b[38;5;241m.\u001b[39mloc[:,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoDescription_stopwords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m      5\u001b[0m filtered_dates \u001b[38;5;241m=\u001b[39m filtered[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "filtered = pd.read_csv(\"filtered_timeseries_data.csv\")\n",
    "filtered_data = filtered.loc[:,\"coDescription_stopwords\"].to_list()\n",
    "filtered_dates = filtered[\"reportingDate\"].to_list()\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.85, max_features=2000) \n",
    "filtered_all_X = tf_vectorizer.fit_transform(filtered_data)\n",
    "\n",
    "nmf_model = NMF(n_components=18, init='nndsvd', random_state=0)\n",
    "nmf_feature = nmf_model.fit_transform(filtered_all_X)\n",
    "nmf_component =nmf_model.components_\n",
    "\n",
    "nmf_topics = std_func.get_topics(nmf_model,tf_vectorizer, 18)\n",
    "nmf_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NMF - Netflix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m netflix_X \u001b[38;5;241m=\u001b[39m \u001b[43mtf_vectorizer\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(netflix[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoDescription\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      2\u001b[0m netflix_top \u001b[38;5;241m=\u001b[39m nmf_model\u001b[38;5;241m.\u001b[39mtransform(netflix_X)\n\u001b[1;32m      3\u001b[0m netflix_top_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(netflix_top)\u001b[38;5;241m.\u001b[39mset_index(netflix[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "netflix_X = tf_vectorizer.transform(netflix[\"coDescription\"].tolist())\n",
    "netflix_top = nmf_model.transform(netflix_X)\n",
    "netflix_top_df = pd.DataFrame(netflix_top).set_index(netflix[\"reportingDate\"])\n",
    "std_func.graph_netflix(18, netflix_top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#compare 2011 with 2006\u001b[39;00m\n\u001b[1;32m      3\u001b[0m std_func\u001b[38;5;241m.\u001b[39mget_differences(nmf_topics, netflix_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], netflix_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "#compare 2011 with 2006\n",
    "std_func.get_differences(nmf_topics, netflix_top_df.iloc[-1], netflix_top_df.iloc[0]).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decrease in topic 2(retail/store), 15(real estate/residential)\n",
    "- increase in topic 10 (finances), 11 (food), 16(common financial report terms)\n",
    "\n",
    "Analysis: \n",
    "\n",
    "Decrease in topic 2(retail/store) and 15(real estate/residential) may be a result of Netflix's change in business model in 2007 which hugely emphasized moving into the video streaming industry and being able to watch content in the comfort of your own home. However, in 2014, the business model is already established so there is a decrease on the emphasis of these topics. \n",
    "\n",
    "Topics 11,16, 10 are too general to be interpreted or is irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NMF - General Electric Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ge_X \u001b[38;5;241m=\u001b[39m \u001b[43mtf_vectorizer\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(ge[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoDescription\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      2\u001b[0m ge_top \u001b[38;5;241m=\u001b[39m nmf_model\u001b[38;5;241m.\u001b[39mtransform(ge_X)\n\u001b[1;32m      3\u001b[0m ge_top_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(ge_top)\u001b[38;5;241m.\u001b[39mset_index(ge[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "ge_X = tf_vectorizer.transform(ge[\"coDescription\"].tolist())\n",
    "ge_top = nmf_model.transform(ge_X)\n",
    "ge_top_df = pd.DataFrame(ge_top).set_index(ge[\"reportingDate\"])\n",
    "std_func.graph_ge(18, ge_top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# #compare 2014 to 2011 \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstd_func\u001b[49m\u001b[38;5;241m.\u001b[39mget_differences(nmf_topics, ge_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], ge_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'std_func' is not defined"
     ]
    }
   ],
   "source": [
    "# #compare 2014 to 2011 \n",
    "std_func.get_differences(nmf_topics, ge_top_df.iloc[-1], ge_top_df.iloc[0]).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decrease in topic 15(real estate/land), 18(aerospace, vehicles)\n",
    "- increase in topic 5(loan/bank), 6(energy/gas), 16 (financial/analysis)\n",
    "\n",
    "Analysis: \n",
    "\n",
    "Decrease in topic 18(aerospace/vehicles) may indicate that the company is seeing steady growth in the Aerospace section and did not make major changes in their business model. Decrease in topic 15(real estate/residential) may be explained by the planned acquisition of GE Appliances by Electrolux. \n",
    "\n",
    "Increase in Topic 5 (loan/bank) may be explained by its acquistion activities where GE Power acquired Alcom and GE Appliances is set to be acquired by Electrolux. Increase in Topic 6(energy/gas) may be explained by GE Power's plan to acquire Alcom. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "##### LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below illustrates the results produced by the LSA model tuned to generate 20 topics. Each column is a topic identified by the column index and is represented by the top 10 words in the topic by weight. It is interesting to note that the topics here are quite difficult to interpret as there are several different categories of words in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD\n\u001b[1;32m      2\u001b[0m svd \u001b[38;5;241m=\u001b[39m TruncatedSVD(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m svd_model \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(svd\u001b[38;5;241m.\u001b[39mfit_transform(filtered_all_X))\n\u001b[1;32m      4\u001b[0m lsa_topics \u001b[38;5;241m=\u001b[39m std_func\u001b[38;5;241m.\u001b[39mget_topics(svd,tf_vectorizer, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      5\u001b[0m lsa_topics\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "svd_model = pd.DataFrame(svd.fit_transform(filtered_all_X))\n",
    "lsa_topics = std_func.get_topics(svd,tf_vectorizer, 20)\n",
    "lsa_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "###### LSA - Netflix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netflix_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m netflix_top \u001b[38;5;241m=\u001b[39m svd\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mnetflix_X\u001b[49m)\n\u001b[1;32m      2\u001b[0m netflix_top_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(netflix_top)\u001b[38;5;241m.\u001b[39mset_index(netflix[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m std_func\u001b[38;5;241m.\u001b[39mgraph_netflix(\u001b[38;5;241m20\u001b[39m, netflix_top_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netflix_X' is not defined"
     ]
    }
   ],
   "source": [
    "netflix_top = svd.transform(netflix_X)\n",
    "netflix_top_df = pd.DataFrame(netflix_top).set_index(netflix[\"reportingDate\"])\n",
    "std_func.graph_netflix(20, netflix_top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#compare 2011 with 2006\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstd_func\u001b[49m\u001b[38;5;241m.\u001b[39mget_differences(lsa_topics,netflix_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], netflix_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'std_func' is not defined"
     ]
    }
   ],
   "source": [
    "#compare 2011 with 2006\n",
    "std_func.get_differences(lsa_topics,netflix_top_df.iloc[-1], netflix_top_df.iloc[0]).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decrease in topic 2(retail/store) ,8(software, mining), 10(loan, home, commodities, cannabis), 13(investment, cannabis)\n",
    "- increase in topic 17(mining, financial)\n",
    "\n",
    "Analysis: \n",
    "\n",
    "Decrease in topic 2(retail/store) and 8(software, mining) may be a result of Netflix's change in business model in 2007 which hugely emphasized moving into the video streaming industry and being able to watch content in the comfort of your own home. However, in 2014, the business model is already established so there is a decrease on the emphasis of these topics.\n",
    "\n",
    "Topics 10, 13, 17 are too general to be interpreted, or is irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSA - General Electric Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ge_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ge_top \u001b[38;5;241m=\u001b[39m svd\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mge_X\u001b[49m)\n\u001b[1;32m      2\u001b[0m ge_top_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(ge_top)\u001b[38;5;241m.\u001b[39mset_index(ge[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m std_func\u001b[38;5;241m.\u001b[39mgraph_ge(\u001b[38;5;241m20\u001b[39m, ge_top_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ge_X' is not defined"
     ]
    }
   ],
   "source": [
    "ge_top = svd.transform(ge_X)\n",
    "ge_top_df = pd.DataFrame(ge_top).set_index(ge[\"reportingDate\"])\n",
    "std_func.graph_ge(20, ge_top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# compare 2014 to 2011 \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstd_func\u001b[49m\u001b[38;5;241m.\u001b[39mget_differences(lsa_topics,ge_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],ge_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'std_func' is not defined"
     ]
    }
   ],
   "source": [
    "# compare 2014 to 2011 \n",
    "std_func.get_differences(lsa_topics,ge_top_df.iloc[-1],ge_top_df.iloc[0]).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decrease in topic 18(aerospace)\n",
    "- increase in topic 14(business, acquisition), 17(mining, financial), 19(food/store), 20(vehicle, mineral, partner)\n",
    "\n",
    "Analysis: \n",
    "\n",
    "Decrease in topic 18(aerospace/vehicles) may indicate that the company is seeing steady growth in the Aerospace section and did not make major changes in their business model. \n",
    "\n",
    "Increase in Topic 14(business, acquisition) may be explained by its increased acquistion activities where GE Power acquired Alcom and GE Appliances is set to be acquired by Electrolux.\n",
    "\n",
    "Topic 17, 19, 20 are too general to be interpreted or is irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the coherence score benchmarking over a range of 3 to 40 topics, incrementing by 3 and achieved the result below. We chose the give our model 9 topics, which has the highest coherence score. If you're interested in the code, see this  [file](https://richardye101.github.io/ubineer_nlp_research/lab?path=mary%2FWeek+10+-+NMF+vs+LDA+vs+LSA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NMF Coherence](../images/lda_coherence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below illustrates the results produced by the LDA model tuned to generate 9 topics. Each column is a topic identified by the column index and is represented by the top 10 words in the topic by weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m filtered_all_count_X \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mfiltered_data\u001b[49m)\n\u001b[1;32m      7\u001b[0m count_feature_names \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names()\n\u001b[1;32m      8\u001b[0m lda \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(filtered_all_count_X)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "count_vectorizer = CountVectorizer(max_df=0.85, min_df=2, max_features=2000)\n",
    "filtered_all_count_X = count_vectorizer.fit_transform(filtered_data)\n",
    "count_feature_names = count_vectorizer.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_components=9,random_state=0).fit(filtered_all_count_X)\n",
    "lda_topics = std_func.get_topics(lda,count_vectorizer, 9)\n",
    "lda_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LDA - Netflix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netflix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m netlfix_X \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mnetflix\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoDescription\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      2\u001b[0m netflix_top \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mtransform(netlfix_X)\n\u001b[1;32m      3\u001b[0m netflix_top_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(netflix_top)\u001b[38;5;241m.\u001b[39mset_index(netflix[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netflix' is not defined"
     ]
    }
   ],
   "source": [
    "netlfix_X = count_vectorizer.transform(netflix[\"coDescription\"].tolist())\n",
    "netflix_top = lda.transform(netlfix_X)\n",
    "netflix_top_df = pd.DataFrame(netflix_top).set_index(netflix[\"reportingDate\"])\n",
    "std_func.graph_netflix(9, netflix_top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#compare 2011 with 2006\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstd_func\u001b[49m\u001b[38;5;241m.\u001b[39mget_differences(lda_topics,netflix_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], netflix_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'std_func' is not defined"
     ]
    }
   ],
   "source": [
    "#compare 2011 with 2006\n",
    "std_func.get_differences(lda_topics,netflix_top_df.iloc[-1], netflix_top_df.iloc[0]).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decrease in topic 1(common financial terms) ,5(software)\n",
    "- increase in topic 6(common financial terms), 7(retail/branding), 9(managment positions?)\n",
    "\n",
    "Analysis: \n",
    "\n",
    "Decrease in topic 5 (software) may be a result of Netflix's change in business model in 2007 which hugely emphasized moving into the video streaming/software industry. However, in 2014, the business model is already established so there is a decrease on the emphasis of software.\n",
    "\n",
    "Topics 1,6,7,9 are too general to be interpreted or is irrelevant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LDA - General Electric Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ge_X \u001b[38;5;241m=\u001b[39m  count_vectorizer\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mge\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoDescription\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      2\u001b[0m ge_top \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mtransform(ge_X )\n\u001b[1;32m      3\u001b[0m ge_top_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(ge_top)\u001b[38;5;241m.\u001b[39mset_index(ge[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ge' is not defined"
     ]
    }
   ],
   "source": [
    "ge_X =  count_vectorizer.transform(ge[\"coDescription\"].tolist())\n",
    "ge_top = lda.transform(ge_X )\n",
    "ge_top_df = pd.DataFrame(ge_top).set_index(ge[\"reportingDate\"])\n",
    "std_func.graph_ge(9, ge_top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# compare 2014 to 2011 \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstd_func\u001b[49m\u001b[38;5;241m.\u001b[39mget_differences(lda_topics,ge_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],ge_top_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'std_func' is not defined"
     ]
    }
   ],
   "source": [
    "# compare 2014 to 2011 \n",
    "std_func.get_differences(lda_topics,ge_top_df.iloc[-1],ge_top_df.iloc[0]).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decrease in topic 9(managment positions?)\n",
    "- increase in topic 7(retail/branding), 8(finance, loan), 1(common financial report terms), 6(positive financial report terms)\n",
    "\n",
    "Analysis: \n",
    "Topics are too general to be interpreted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary of Topic Modelling\n",
    "##### General Analysis\n",
    "In all three models, we observe an interesting decrease in the software topic (Topic 8 in NMF, Topic 7/8 in LSA, and Topic 5 in LDA)  from Netflix from 2006 to 2011. This is surprising because we expected more mentions of software terms after Netflix entered the video streaming market in 2007 and especially in 2011 when Netflix rolled out mobile apps for smartphone users. \n",
    "\n",
    "In the NMF and LSA models, we observe an interesting decrease in the aerospace topic (Topic 18 in NMF, Topic 18 in LSA) from GE from 2011 to 2014. This is surprising because GE Aviation was the most profitable sector of GE during these 3 years so we expected more mentions of aerospace terms.\n",
    "\n",
    "From these observations, we can take away that an increase in the mentions of the words in a topic does not necessarily mean the company business model is moving more in the direction of that topic. Similarly, a decrease in the mentions of the words in a topic does not necessarily mean the company's business model is diverging from that topic. However, we can say that any significant increase or decrease in a topic will give an indication that there has been a change in the company business model with regard to the topic in discussion. \n",
    "\n",
    "##### NMF Results\n",
    "__Netflix__\n",
    "- decrease in topic 2(retail/store), 15(real estate/residential)\n",
    "- increase in topic 10 (finances), 11 (food), 16(common financial report terms)\n",
    "\n",
    "__GE__\n",
    "- decrease in topic 15(real estate/land), 18(aerospace, vehicles)\n",
    "- increase in topic 5(loan/bank), 6(energy/gas), 16 (financial/analysis)\n",
    "\n",
    "##### LDA Results\n",
    "\n",
    "__Netflix__\n",
    "- decrease in topic 1(common financial terms) ,5(software)\n",
    "- increase in topic 6(common financial terms), 7(retail/branding), 9(managment positions?)\n",
    "\n",
    "__GE__\n",
    "- decrease in topic 9(managment positions?)\n",
    "- increase in topic 7(retail/branding), 8(finance, loan), 1(common financial report terms), 6(positive financial report terms)\n",
    "\n",
    "##### LSA Results\n",
    "\n",
    "__Netflix__\n",
    "- decrease in topic 2(retail/store) ,8(software, mining), 10(loan, home, commodities, cannabis), 13(investment, cannabis)\n",
    "- increase in topic 17(mining, financial)\n",
    "\n",
    "__GE__\n",
    "- decrease in topic 18(aerospace)\n",
    "- increase in topic 14(business, acquisition), 17(mining, financial), 19(food/store), 20(vehicle, mineral, partner)\n",
    "\n",
    "Topics generated by the NMF model are the easiest to evaluate and are more coherent compared to LDA and LSA. We saw that the LSA model generated topics with mixed categories of words in each topic and contained negative weights which are difficult to interpret.  The LDA model generated many topics with common financial terms that appear in most 10k reports so it did not give meaningful information about each company. Therefore, NMF does the best using this dataset which is expected since NMF usually has higher performance than LDA and LSA when using a small dataset.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
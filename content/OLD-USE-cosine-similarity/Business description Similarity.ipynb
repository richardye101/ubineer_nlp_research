{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import gensim\n",
    "import pandas\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(html_file):\n",
    "    with open(html_file, encoding=\"utf-8\") as fp:\n",
    "        soup = BeautifulSoup(fp, 'html.parser').get_text(strip=True)\n",
    "    text= unicodedata.normalize(\"NFKD\",soup)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# file_lst_okta=[r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\Enterprise Identity Security\\OKTA\\okta-1312018_10k.htm\", r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\Enterprise Identity Security\\OKTA\\okta-1312019_10k.htm\",r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\Enterprise Identity Security\\OKTA\\okta-131202010k.htm\",r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\Enterprise Identity Security\\OKTA\\okta-20210131.htm\"]\n",
    "# file_lst_sail = [r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\Enterprise Identity Security\\SAIL\\sail-10k_20171231.htm\",r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\Enterprise Identity Security\\SAIL\\sail-10k_20181231.htm\",r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\Enterprise Identity Security\\SAIL\\sail-10k_20191231.htm\",r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\Enterprise Identity Security\\SAIL\\sail-20201231.htm\"]\n",
    "# file_lst_aapl = [r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\AAPL\\AAPL2017.htm\",r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\AAPL\\AAPL2018.htm\",r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\AAPL\\AAPL2019.htm\",r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\AAPL\\AAPL2020.htm\" ]\n",
    "# print(len(file_lst_sail))\n",
    "# print(len(file_lst_aapl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_it_okta(file_lst):\n",
    "    co_business ={}\n",
    "    for i in file_lst:\n",
    "        no_html_txt=remove_html_tags(i)\n",
    "        item_symbol=re.compile('Item..\\.')\n",
    "        item_data = item_symbol.split(no_html_txt)\n",
    "        #print(len(item_data))\n",
    "        lst_bus=[]\n",
    "        for ele in item_data:\n",
    "            if 'Business' in ele[0:20]:\n",
    "                lst_bus.append(ele)\n",
    "        #print('--------'+i+'---------')\n",
    "        result=lst_bus[-1][0:lst_bus[-1].find(\"Item 1A\")]\n",
    "        co_business[i[:-4]]=result\n",
    "    return co_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_it_sail(file_lst):\n",
    "    co_business ={}\n",
    "    for i in file_lst:\n",
    "        print(i)\n",
    "        no_html_txt=remove_html_tags(i)\n",
    "        item_symbol=re.compile('ITEM..\\.')\n",
    "        item_data = item_symbol.split(no_html_txt)\n",
    "        #print(len(item_data))\n",
    "        lst_bus=[]\n",
    "        for ele in item_data:\n",
    "            if ('Business' in ele[0:20] or 'BUSINESS' in ele[0:20]):\n",
    "                lst_bus.append(ele)\n",
    "            \n",
    "\n",
    "        #print('--------'+i+'---------')\n",
    "        #print(item_data)\n",
    "        result=lst_bus[-1][0:lst_bus[-1].find(\"ITEM 1A\")]\n",
    "        co_business[i[:-4]]=result\n",
    "    return co_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_it_aapl(file_lst):\n",
    "    co_business ={}\n",
    "    for i in file_lst:\n",
    "        no_html_txt=remove_html_tags(i)\n",
    "        item_symbol=re.compile('Item..\\.')\n",
    "        item_data = item_symbol.split(no_html_txt)\n",
    "        #print(len(item_data))\n",
    "        lst_bus=[]\n",
    "        for ele in item_data:\n",
    "            if 'Business' in ele[0:20]:\n",
    "                lst_bus.append(ele)\n",
    "        #print('--------'+i+'---------')\n",
    "        result=lst_bus[-1][0:lst_bus[-1].find(\"Item 1A\")]\n",
    "        co_business[i[:-4]]=result\n",
    "    return co_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\andik\\\\Desktop\\\\GITLAB\\\\testing-nlp\\\\Enterprise Identity Security\\\\OKTA\\\\okta-1312018_10k.htm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m okta \u001b[38;5;241m=\u001b[39m \u001b[43mrun_it_okta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_lst_okta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_okta\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(okta,orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBD\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m df_okta\u001b[38;5;241m.\u001b[39mhead()\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mrun_it_okta\u001b[0;34m(file_lst)\u001b[0m\n\u001b[1;32m      2\u001b[0m co_business \u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m file_lst:\n\u001b[0;32m----> 4\u001b[0m     no_html_txt\u001b[38;5;241m=\u001b[39m\u001b[43mremove_html_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     item_symbol\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItem..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     item_data \u001b[38;5;241m=\u001b[39m item_symbol\u001b[38;5;241m.\u001b[39msplit(no_html_txt)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mremove_html_tags\u001b[0;34m(html_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_html_tags\u001b[39m(html_file):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhtml_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m      3\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(fp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     text\u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNFKD\u001b[39m\u001b[38;5;124m\"\u001b[39m,soup)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\andik\\\\Desktop\\\\GITLAB\\\\testing-nlp\\\\Enterprise Identity Security\\\\OKTA\\\\okta-1312018_10k.htm'"
     ]
    }
   ],
   "source": [
    "# okta = run_it_okta(file_lst_okta)\n",
    "# df_okta=pd.DataFrame.from_dict(okta,orient='index',columns=['BD'])\n",
    "# df_okta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sail = run_it_sail(file_lst_sail)\n",
    "# df_sail=pd.DataFrame.from_dict(sail,orient='index',columns=['BD'])\n",
    "# df_sail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aapl = run_it_aapl(file_lst_aapl)\n",
    "# df_aapl=pd.DataFrame.from_dict(aapl,orient='index',columns=['BD'])\n",
    "# df_aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing words\n",
    "nltk_tokens_okta = nltk.word_tokenize(df_okta[\"BD\"][-1])\n",
    "tokens_okta = [token.lower() for token in nltk_tokens_okta]\n",
    "nltk_tokens_sail = nltk.word_tokenize(df_sail[\"BD\"][-1])\n",
    "tokens_sail = [token.lower() for token in nltk_tokens_sail]\n",
    "nltk_tokens_aapl = nltk.word_tokenize(df_aapl[\"BD\"][-1])\n",
    "tokens_aapl = [token.lower() for token in nltk_tokens_aapl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(tokens):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filtered_with_char = [w for w in tokens if not w.lower() in stop_words]\n",
    "    #filtered_with_char\n",
    "    filtered_with_dates_time=[word.lower() for word in filtered_with_char if word.isalpha()]\n",
    "    #filtered_with_dates_time\n",
    "    file_content = open(r\"C:\\Users\\andik\\Desktop\\GITLAB\\testing-nlp\\StopWords_DatesandNumbers.txt\")\n",
    "    stop_word_tokens_date = nltk.word_tokenize(file_content.read())\n",
    "    file_content.close()\n",
    "    stop_word_tokens_date =  [token.lower() for token in stop_word_tokens_date]\n",
    "    #stop_word_tokens_date\n",
    "\n",
    "    filtered = [w for w in filtered_with_dates_time if not w.lower() in set(stop_word_tokens_date)]\n",
    "    #filtered\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0]\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    root_words =[]\n",
    "\n",
    "    for w in range(len(filtered)):\n",
    "        root_words.append(lemmatizer.lemmatize(filtered[w], get_wordnet_pos(filtered[w])))\n",
    "    \n",
    "    fdist = FreqDist(root_words)\n",
    "    \n",
    "    return(fdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_okta= filter(tokens_okta)\n",
    "filtered_sail= filter(tokens_sail)\n",
    "filtered_aapl= filter(tokens_aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_cmmn_words_okta=filtered_okta.most_common(30)\n",
    "most_cmmn_words_okta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_cmmn_words_sail=filtered_sail.most_common(30)\n",
    "most_cmmn_words_sail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_cmmn_words_aapl=filtered_aapl.most_common(30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lst(lists):\n",
    "    new_lst = []\n",
    "    for ele in lists:\n",
    "        new_lst.append(ele[0])\n",
    "    return(new_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds_okta= word_lst(most_cmmn_words_okta)\n",
    "wrds_sail= word_lst(most_cmmn_words_sail)\n",
    "wrds_aapl= word_lst(most_cmmn_words_aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard(wrds_okta,wrds_sail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_test(filtered_okta,filtered_sail):\n",
    "    jaccard_lst=[]\n",
    "    ele_lst=[]\n",
    "    for i in range(60):\n",
    "        i=i+5\n",
    "        ele_lst.append(i)\n",
    "        most_cmmn_words_okta=filtered_okta.most_common(i)\n",
    "        most_cmmn_words_sail=filtered_sail.most_common(i)\n",
    "        wrds_okta= word_lst(most_cmmn_words_okta)\n",
    "        wrds_sail= word_lst(most_cmmn_words_sail)\n",
    "        jaccard_lst.append(jaccard(wrds_okta,wrds_sail))\n",
    "    \n",
    "    d={\"#\":ele_lst,\"jaccard\":jaccard_lst }\n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return df      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = jaccard_test(filtered_okta,filtered_sail)\n",
    "\n",
    "a.plot(x=\"#\",y=\"jaccard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(df_okta,df_sail):\n",
    "    \n",
    "    rslt_lst=[]\n",
    "\n",
    "    for ele in range(len(df_okta[\"BD\"])):\n",
    "        nltk_tokens_okta = nltk.word_tokenize(df_okta[\"BD\"][ele])\n",
    "        tokens_okta = [token.lower() for token in nltk_tokens_okta]\n",
    "        nltk_tokens_sail = nltk.word_tokenize(df_sail[\"BD\"][ele])\n",
    "        tokens_sail = [token.lower() for token in nltk_tokens_sail]\n",
    "        filtered_okta= filter(tokens_okta)\n",
    "        filtered_sail= filter(tokens_sail)\n",
    "\n",
    "        rslt_lst.append(jaccard_test(filtered_okta,filtered_sail))\n",
    "    \n",
    "    return(rslt_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=run_all(df_okta,df_sail)\n",
    "for r in results:\n",
    "    print(r.plot(x=\"#\",y=\"jaccard\"))\n",
    "    print(\"AVG:\")\n",
    "    print(r[\"jaccard\"].mean())\n",
    "    print(\"SD:\")\n",
    "    print(r[\"jaccard\"].std())\n",
    "    print(\"SKEW:\")\n",
    "    print(r[\"jaccard\"].skew())\n",
    "\n",
    "results_okta_aapl=run_all(df_okta,df_aapl)\n",
    "for r in results_okta_aapl:\n",
    "    print(r.plot(x=\"#\",y=\"jaccard\"))\n",
    "    print(\"AVG:\")\n",
    "    print(r[\"jaccard\"].mean())\n",
    "    print(\"SD:\")\n",
    "    print(r[\"jaccard\"].std())\n",
    "    print(\"SKEW:\")\n",
    "    print(r[\"jaccard\"].skew())\n",
    "\n",
    "results_sail_aapl=run_all(df_sail,df_aapl)\n",
    "for r in results_sail_aapl:\n",
    "    print(r.plot(x=\"#\",y=\"jaccard\"))\n",
    "    print(\"AVG:\")\n",
    "    print(r[\"jaccard\"].mean())\n",
    "    print(\"SD:\")\n",
    "    print(r[\"jaccard\"].std())\n",
    "    print(\"SKEW:\")\n",
    "    print(r[\"jaccard\"].skew())\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIND Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(most_cmmn_words_okta)\n",
    "#print(most_cmmn_words_sail)\n",
    "# def unique_words(most_cmmn_words_okta,most_cmmn_words_sail):\n",
    "#     words_lst= []\n",
    "#     most_cmmn_lst=[most_cmmn_words_sail,most_cmmn_words_okta]\n",
    "#     for ele in most_cmmn_lst:\n",
    "#         for i in ele:\n",
    "\n",
    "df = pd.DataFrame(most_cmmn_words_okta, columns=[\"Words\", \"Count_OKTA\"])\n",
    "df=df.set_index(\"Words\")\n",
    "df1 = pd.DataFrame(most_cmmn_words_sail, columns=[\"Words\", \"Count_SAIL\"])\n",
    "df1=df1.set_index(\"Words\")\n",
    "df2 = pd.DataFrame(most_cmmn_words_aapl,columns=[\"Words\", \"Count_AAPL\"])\n",
    "df2 =df2.set_index(\"Words\")\n",
    "#result= pd.merge(df,df1,left_index=True, right_index=True )\n",
    "#result=df.join(df1)\n",
    "result=pd.concat([df1, df,df2], axis=1).fillna(0)\n",
    "result=result.transpose()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity_matrix = cosine_similarity(result)\n",
    "cosine_similarity_matrix[0][-1]\n",
    "cosine_similarity_matrix[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_fn(filtered_okta,filtered_sail,filtered_aapl):\n",
    "    cos_sim_matrix_aapl=[]\n",
    "    cos_sim_matrix_sail=[]\n",
    "    ele_lst=[]\n",
    "    for i in range(150):\n",
    "        i=i+5\n",
    "        ele_lst.append(i)\n",
    "        most_cmmn_words_okta=filtered_okta.most_common(i)\n",
    "        most_cmmn_words_sail=filtered_sail.most_common(i)\n",
    "        most_cmmn_words_aapl=filtered_aapl.most_common(i)\n",
    "        df = pd.DataFrame(most_cmmn_words_okta, columns=[\"Words\", \"Count_OKTA\"])\n",
    "        df=df.set_index(\"Words\")\n",
    "        df1 = pd.DataFrame(most_cmmn_words_sail, columns=[\"Words\", \"Count_SAIL\"])\n",
    "        df1=df1.set_index(\"Words\")\n",
    "        df2 = pd.DataFrame(most_cmmn_words_aapl,columns=[\"Words\", \"Count_AAPL\"])\n",
    "        df2 =df2.set_index(\"Words\")\n",
    "        #result= pd.merge(df,df1,left_index=True, right_index=True )\n",
    "        #result=df.join(df1)\n",
    "        result=pd.concat([df1, df,df2], axis=1).fillna(0)\n",
    "        result=result.transpose()\n",
    "        cosine_similarity_matrix = cosine_similarity(result)\n",
    "        cos_sim_matrix_aapl.append(cosine_similarity_matrix[0][-1])\n",
    "        cos_sim_matrix_sail.append(cosine_similarity_matrix[0][1])\n",
    "    d={\"#\":ele_lst, \"cos_sim_sail\": cos_sim_matrix_sail,\"cos_sim_aapl\":cos_sim_matrix_aapl, }\n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return df      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cos_sim_results= cos_sim_fn(filtered_okta,filtered_sail,filtered_aapl)\n",
    "print(cos_sim_results)\n",
    "def run_all_cos_sim(df_okta,df_sail,df_aapl):\n",
    "    \n",
    "    rslt_lst=[]\n",
    "\n",
    "    for ele in range(len(df_okta[\"BD\"])):\n",
    "        nltk_tokens_okta = nltk.word_tokenize(df_okta[\"BD\"][ele])\n",
    "        tokens_okta = [token.lower() for token in nltk_tokens_okta]\n",
    "        nltk_tokens_sail = nltk.word_tokenize(df_sail[\"BD\"][ele])\n",
    "        tokens_sail = [token.lower() for token in nltk_tokens_sail]\n",
    "        nltk_tokens_aapl = nltk.word_tokenize(df_aapl[\"BD\"][ele])\n",
    "        tokens_sail = [token.lower() for token in nltk_tokens_aapl]\n",
    "        filtered_okta= filter(tokens_okta)\n",
    "        filtered_sail= filter(tokens_sail)\n",
    "        filtered_aapl= filter(tokens_aapl)\n",
    "        rslt_lst.append(cos_sim_fn(filtered_okta,filtered_sail,filtered_aapl))\n",
    "    \n",
    "    return(rslt_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all_cos_sim = run_all_cos_sim(df_okta,df_sail,df_aapl)\n",
    "print(result_all_cos_sim)\n",
    "\n",
    "# for r in result_all_cos_sim:\n",
    "#     print(r.plot(x=\"#\",y=\"cos_sim\"))\n",
    "#     print(\"AVG:\")\n",
    "#     print(r[\"cos_sim\"].mean())\n",
    "#     print(\"SD:\")\n",
    "#     print(r[\"cos_sim\"].std())\n",
    "#     print(\"SKEW:\")\n",
    "#     print(r[\"cos_sim\"].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "vector_sail=result.transpose()[\"Count_SAIL\"]\n",
    "vector_okta=result.transpose()[\"Count_OKTA\"]\n",
    "dst = distance.euclidean(vector_sail, vector_okta)\n",
    "dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "r1=result.transpose()\n",
    "r1['new'] = 0\n",
    "sail_cnt=r1[['new',\"Count_SAIL\"]]\n",
    "okta_cnt=r1[['new',\"Count_OKTA\"]]\n",
    "#print(sail_cnt)\n",
    "#print(okta_cnt)\n",
    "\n",
    "tsne = TSNE(n_components=2,random_state=0)\n",
    "tsne1 = TSNE(n_components=2,random_state=0)\n",
    "# print(len(result.transpose()[\"Count_SAIL\"]))\n",
    "tsne_sail = tsne.fit_transform(sail_cnt.transpose())\n",
    "tsne_okta = tsne.fit_transform(okta_cnt.transpose())\n",
    "print(tsne_okta)\n",
    "print(tsne_sail)\n",
    "tsne.fit_transform(r1.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_co = PCA(n_components=2,random_state=0)\n",
    "principalComponents_sail = pca_co.fit_transform(sail_cnt.transpose())\n",
    "principalComponents_okta = pca_co.fit_transform(okta_cnt.transpose())\n",
    "print(sail_cnt.transpose())\n",
    "print(okta_cnt.transpose())\n",
    "r1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(principalComponents_sail)\n",
    "print(principalComponents_sail[0][0])\n",
    "print(principalComponents_sail[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(principalComponents_okta)\n",
    "print(principalComponents_okta[1][0])\n",
    "print(principalComponents_okta[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(principalComponents_sail[0][0],principalComponents_sail[0][1])\n",
    "plt.scatter(principalComponents_okta[1][0],principalComponents_okta[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "407699af088346e4d15b4a0b1fbb498ade0b7d734f379f3dd6d79bae52db9e9b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

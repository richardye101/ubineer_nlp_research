{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bcaa702",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport std_func\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5e5cb6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9148826",
   "metadata": {},
   "source": [
    "## Estimates from Factor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233c60c5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "r_selected = pd.read_csv(\"data/filtered_r.csv\")\n",
    "# get the mean of all \n",
    "r_selected.set_index(\"name\", inplace = True)\n",
    "mu = r_selected.mean(axis = 1)\n",
    "# compute the covariance matrix \n",
    "cov = r_selected.T.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a2e6bf6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocessed.csv',\n",
    "                 usecols = ['reportingDate', 'name', 'CIK', 'coDescription',\n",
    "                           'coDescription_stopwords', 'SIC', 'SIC_desc'])\n",
    "df = df.set_index(df.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ba18e",
   "metadata": {},
   "source": [
    "### Sent-LDA\n",
    "We ran the coherence score benchmarking over a range of 3 to 40 topics, incrementing by 3. First, we fit the LDA model to all business description using the number of topics selected from coherence score benchmarking. Then, we assume each sentence only represents one topic; get the frequency of the topics revealed in the whole document (business description for one company) and calculate the probability of each topics in the whole document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3133d091",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "data = df.loc[:,\"coDescription_stopwords\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaec5e63",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def get_topics(model, vectorizer, num_topics):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-10 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9701604e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 410, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 409, in _send_bytes\n",
      "    self._send(header)\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 409, in _send_bytes\n",
      "    self._send(header)\n",
      "  File \"/Users/lulu/opt/anaconda3/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6t/0lh6qmkn1zg8fhlp984cpxrr0000gn/T/ipykernel_45452/1488680959.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcoherence_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic_nums\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     lda = LdaModel(corpus=corpus,\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m             self.add_lifecycle_event(\n\u001b[1;32m    522\u001b[0m                 \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                     )\n\u001b[0;32m-> 1005\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0;31m# Substituting the value of the optimal phi back into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;31m# the update for gamma gives this update. Cf. Lee&Seung 2001.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0mgammad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mphinorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m                 \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel, TfidfModel\n",
    "\n",
    "dictionary = Dictionary(doc.split() for doc in data)\n",
    " # filter out too in/frequent tokens\n",
    "dictionary.filter_extremes(no_below=0.01, no_above=0.85, keep_n = 5000)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc.split()) for doc in data]\n",
    "# create list of topic number we want to try \n",
    "topic_nums = list(np.arange(3, 40,3))\n",
    "\n",
    "coherence_scores = []\n",
    "for num in topic_nums:\n",
    "    lda = LdaModel(corpus=corpus,\n",
    "    num_topics=num,\n",
    "    id2word=dictionary,\n",
    "    passes=5,\n",
    "    eval_every=10,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=0)\n",
    "    \n",
    "    \n",
    "    cm = CoherenceModel(\n",
    "    model=lda,\n",
    "    texts=[doc.split() for doc in data],\n",
    "    dictionary=dictionary,\n",
    "    coherence=\"c_v\")\n",
    "\n",
    "    coherence_scores.append(round(cm.get_coherence(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1b5c7",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "scores = list(zip(topic_nums, coherence_scores))\n",
    "plt.plot(topic_nums, coherence_scores)\n",
    "plt.xticks(np.arange(3, 40,2))\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.xlabel('Topic Num')\n",
    "plt.title(\"Coherence Score by Topic Number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9198b5",
   "metadata": {},
   "source": [
    "Based on the above Coherence Score, we choose up to 12 topics since it gives the highest score up to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd54c9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.85, min_df=2, max_features=600)\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "lda = LatentDirichletAllocation(n_components=12, random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc40c63",
   "metadata": {},
   "source": [
    "We show the top 10 words by weights in the 12 topics LDA model generates in the below table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7fd95",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "get_topics(lda, tf_vectorizer, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5217a",
   "metadata": {},
   "source": [
    "#### Assign Frequency of the topic to each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b9361",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "n_components = 12\n",
    "prob = pd.DataFrame(0, index = df.name, columns = range(n_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d398240",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for j in range(len(df)):\n",
    "    LIST_sent = pd.Series(df.coDescription[j].split('.')).apply(std_func.lemmatize_sentence).apply(std_func.remove_nums).apply(std_func.remove_stopwords)\n",
    "    \n",
    "    X = tf_vectorizer.transform(LIST_sent.tolist())\n",
    "    sent = lda.transform(X)\n",
    "    sent_df = pd.DataFrame(sent)\n",
    "    # drop the values that are smaller than 1/12\n",
    "    # if the maximum value is 1/12, the probability of each topic in that sentence is the same\n",
    "    # we cannot determine which topic to choose\n",
    "    sent_df = sent_df[sent_df.max(axis = 1) > 1/12].reset_index(drop = True)\n",
    "\n",
    "    for i in range(n_components):\n",
    "        prob.iloc[j][i] = list(sent_df.idxmax(axis = 1)).count(i)\n",
    "    \n",
    "    # calculate the probability\n",
    "    prob = prob.div(prob.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca653cbf",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb03616",
   "metadata": {},
   "source": [
    "#### Demonstrate using pharmaceutical preparations industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d34378",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# get the names of the companies in the pharmaceutical preparations industry\n",
    "Pharm = df[df.SIC == 2834]\n",
    "Pharm_list = Pharm.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884a6af",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# get the companies name that match return data and business description data\n",
    "SET = (set(Pharm_list) & set(r_selected.index))\n",
    "LIST = [*SET, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213c644",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "B_matrix = prob.T[LIST].T\n",
    "B_matrix = B_matrix[~B_matrix.index.duplicated(keep=\"first\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc56bfa",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "r_Pharm = r_selected.T[LIST].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abd0d3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "coef_mat = pd.DataFrame(0, index = r_Pharm.columns, columns = range(n_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403be5b",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LR = LinearRegression()\n",
    "for i in range(len(r_Pharm.columns)):\n",
    "    date = r_Pharm.columns[i]\n",
    "    r_t_i = r_Pharm[date] \n",
    "    reg = LR.fit(B_matrix, r_t_i)\n",
    "    #print(reg.score(B_matrix, r_t_i))\n",
    "    coef_mat.iloc[i] = reg.coef_\n",
    "\n",
    "coef_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbabb94",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "prediction = B_matrix.dot(coef_mat.T)\n",
    "residual = r_Pharm - prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470f0e7",
   "metadata": {},
   "source": [
    "### Perform Mean-Variance Analysis\n",
    "We only use the Pharmaceutical Preparations industry data to generate portfolio based on Mean-Variance Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956d4ec",
   "metadata": {
    "tags": [
     "remove-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "pip install dataframe_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68e743",
   "metadata": {
    "tags": [
     "remove-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "pip install PyPortfolioOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ad3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import objective_functions\n",
    "from pypfopt import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fda2eb",
   "metadata": {},
   "source": [
    "#### Mean for Returns in the Pharmaceutical Preparations Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a494b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mu_Pharm = mu[LIST]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42947f8e",
   "metadata": {},
   "source": [
    "#### Covariance generated using the residual of the factor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7dcdb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "cov_Factor_Model = residual.T.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23e564",
   "metadata": {},
   "source": [
    "### Minmum Volatility Portfolio Weights - Factor Model Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b6a4c",
   "metadata": {},
   "source": [
    "#### Comparing return mean and covariance generated using factor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429ef48",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "ef1 = EfficientFrontier(mu_Pharm, cov_Factor_Model, weight_bounds=(0, 0.2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plotting.plot_efficient_frontier(ef1, ax=ax, show_assets=True)\n",
    "\n",
    "# Find and plot the tangency portfolio\n",
    "ef2 = EfficientFrontier(mu_Pharm, cov_Factor_Model, weight_bounds=(0, 0.2))\n",
    "# min volatility\n",
    "ef2.min_volatility()\n",
    "ret_tangent, std_tangent, _ = ef2.portfolio_performance()\n",
    "ax.scatter(std_tangent, ret_tangent, marker=\"*\", s=100, c=\"r\", label=\"Min Volatility\")\n",
    "\n",
    "# Format\n",
    "ax.set_title(\"Efficient Frontier - Pharmaceutical Preparations \\n Factor Model Estimates\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/Efficient_Frontier_Returns.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e7542",
   "metadata": {},
   "source": [
    "##### min volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e4df3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "ef2.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251dc5e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "companies = []\n",
    "weights = []\n",
    "for company, weight in ef2.clean_weights().items():\n",
    "    if weight != 0:\n",
    "        companies.append(company)\n",
    "        weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b18fbf",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "dic = {'Company_Name':companies,'Weight':weights}\n",
    "min_vol = pd.DataFrame(dic)\n",
    "min_vol.to_csv(\"min_vol_factor_model.csv\")\n",
    "import dataframe_image as dfi\n",
    "min_vol.dfi.export('min_vol_factor_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162bb1f9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "min_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d9980",
   "metadata": {},
   "source": [
    "### Get results for the other 4 industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d540854",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "SIC_list = [7372, 1311, 6798, 6022]\n",
    "SIC_desc = ['Prepackaged Software (mass reproduction of software)', 'Crude Petroleum and Natural Gas', \n",
    "           'Real Estate Investment Trusts', 'State Commercial Banks (commercial banking)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f88b2",
   "metadata": {},
   "source": [
    "#### Prepackaged Software (mass reproduction of software)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9924f61",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "SIC = SIC_list[0]\n",
    "    \n",
    "industry_name = SIC_desc[SIC_list.index(SIC)]\n",
    "    \n",
    "# get the names of the companies in the other industries\n",
    "Companies = df[df.SIC == SIC]\n",
    "Company_list = Companies.index\n",
    "\n",
    "# get the companies name that match return data and business description data\n",
    "SET = (set(Company_list) & set(r_selected.index))\n",
    "LIST = [*SET, ]\n",
    "\n",
    "B_matrix = prob.T[LIST].T\n",
    "B_matrix = B_matrix[~B_matrix.index.duplicated(keep=\"first\")]\n",
    "\n",
    "r = r_selected.T[LIST].T\n",
    "\n",
    "coef_mat = pd.DataFrame(0, index = r.columns, columns = range(n_components))\n",
    "\n",
    "LR = LinearRegression()\n",
    "for i in range(len(r.columns)):\n",
    "    date = r.columns[i]\n",
    "    r_t_i = r[date] \n",
    "    reg = LR.fit(B_matrix, r_t_i)\n",
    "    coef_mat.iloc[i] = reg.coef_\n",
    "\n",
    "prediction = B_matrix.dot(coef_mat.T)\n",
    "residual = r - prediction\n",
    "\n",
    "mu_sample = mu[LIST]\n",
    "\n",
    "cov_Factor_Model = residual.T.cov()\n",
    "    \n",
    "# perform minimum variance analysis\n",
    "ef1 = EfficientFrontier(mu_sample, cov_Factor_Model, weight_bounds=(0, 0.2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plotting.plot_efficient_frontier(ef1, ax=ax, show_assets=True)\n",
    "\n",
    "# Find and plot the tangency portfolio\n",
    "ef2 = EfficientFrontier(mu_sample, cov_Factor_Model, weight_bounds=(0, 0.2))\n",
    "# min volatility\n",
    "ef2.min_volatility()\n",
    "ret_tangent, std_tangent, _ = ef2.portfolio_performance()\n",
    "ax.scatter(std_tangent, ret_tangent, marker=\"*\", s=100, c=\"r\", label=\"Min Volatility\")\n",
    "\n",
    "# Format\n",
    "ax.set_title(\"Efficient Frontier - %s \\n Factor Model Estimates\" %industry_name)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/Efficient_Frontier_Factor_Model_Estimates' + str(industry_name) + '.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf98d16",
   "metadata": {},
   "source": [
    "##### min volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7bd7b",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "ef2.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e276a",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "companies = []\n",
    "weights = []\n",
    "for company, weight in ef2.clean_weights().items():\n",
    "    if weight != 0:\n",
    "        companies.append(company)\n",
    "        weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7c3ae",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "dic = {'Company_Name':companies,'Weight':weights}\n",
    "min_vol = pd.DataFrame(dic)\n",
    "min_vol.to_csv(\"min_vol_factor_model\" + str(industry_name) + \".csv\")\n",
    "import dataframe_image as dfi\n",
    "min_vol.dfi.export('min_vol_factor_model' + str(industry_name) + '.png')\n",
    "min_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b259a",
   "metadata": {},
   "source": [
    "#### State Commercial Banks (commercial banking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0887871",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "SIC = SIC_list[3]\n",
    "    \n",
    "industry_name = SIC_desc[SIC_list.index(SIC)]\n",
    "    \n",
    "# get the names of the companies in the other industries\n",
    "Companies = df[df.SIC == SIC]\n",
    "Company_list = Companies.index\n",
    "\n",
    "# get the companies name that match return data and business description data\n",
    "SET = (set(Company_list) & set(r_selected.index))\n",
    "LIST = [*SET, ]\n",
    "\n",
    "B_matrix = prob.T[LIST].T\n",
    "B_matrix = B_matrix[~B_matrix.index.duplicated(keep=\"first\")]\n",
    "\n",
    "r = r_selected.T[LIST].T\n",
    "\n",
    "coef_mat = pd.DataFrame(0, index = r.columns, columns = range(n_components))\n",
    "\n",
    "LR = LinearRegression()\n",
    "for i in range(len(r.columns)):\n",
    "    date = r.columns[i]\n",
    "    r_t_i = r[date] \n",
    "    reg = LR.fit(B_matrix, r_t_i)\n",
    "    coef_mat.iloc[i] = reg.coef_\n",
    "\n",
    "prediction = B_matrix.dot(coef_mat.T)\n",
    "residual = r - prediction\n",
    "\n",
    "mu_sample = mu[LIST]\n",
    "\n",
    "cov_Factor_Model = residual.T.cov()\n",
    "\n",
    "\n",
    "# perform minimum variance analysis\n",
    "ef1 = EfficientFrontier(mu_sample, cov_Factor_Model, weight_bounds=(0, 0.2))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plotting.plot_efficient_frontier(ef1, ax=ax, show_assets=True)\n",
    "\n",
    "# Find and plot the tangency portfolio\n",
    "ef2 = EfficientFrontier(mu_sample, cov_Factor_Model, weight_bounds=(0, 0.2))\n",
    "# min volatility\n",
    "ef2.min_volatility()\n",
    "ret_tangent, std_tangent, _ = ef2.portfolio_performance()\n",
    "ax.scatter(std_tangent, ret_tangent, marker=\"*\", s=100, c=\"r\", label=\"Min Volatility\")\n",
    "\n",
    "# Format\n",
    "ax.set_title(\"Efficient Frontier - %s \\n Factor Model Estimates\" %industry_name)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/Efficient_Frontier_Factor_Model_Estimates' + str(industry_name) + '.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd6beb",
   "metadata": {},
   "source": [
    "##### min volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3386a15",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "ef2.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c00f7e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "companies = []\n",
    "weights = []\n",
    "for company, weight in ef2.clean_weights().items():\n",
    "    if weight != 0:\n",
    "        companies.append(company)\n",
    "        weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926796f",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "dic = {'Company_Name':companies,'Weight':weights}\n",
    "min_vol = pd.DataFrame(dic)\n",
    "min_vol.to_csv(\"min_vol_factor_model\" + str(industry_name) + \".csv\")\n",
    "import dataframe_image as dfi\n",
    "min_vol.dfi.export('min_vol_factor_model' + str(industry_name) + '.png')\n",
    "min_vol"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07436507-b3a7-49ff-86ec-bae209ae6e2d",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "source": [
    "# Embedding methods to augment with dynamic analysis\n",
    "\n",
    "This notebook aims to look at the word embeddings for companies and look at whats changed between the years 2016 to 2018 in terms of word.\n",
    "<!-- - tf-idf (term frequency - inverse document frequency)\n",
    "- LDA (Latent Dirichlet Allocation)\n",
    "- LSA (Latent Semantic Analysis)\n",
    "- word2vec\n",
    "- doc2vec -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976d94f9-3448-4382-8e39-2482dda4cadb",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  \n",
    "sys.path.insert(0, '..')\n",
    "import std_func\n",
    "\n",
    "data = pd.read_csv(\"../data/preprocessed.csv\")\n",
    "data_t = pd.read_csv(\"../data/timeseries_data_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c210a-2e3f-470e-86d6-2be173aac1e9",
   "metadata": {},
   "source": [
    "Here are the five companies we wish to look at as a proof of concept: `OKTA`, `Z-Scaler`, `NFLX`, `IBM` and `GE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d60cd3e-a212-4d5d-b232-1ec47ef3d375",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filingDate</th>\n",
       "      <th>financialEntity</th>\n",
       "      <th>coDescription</th>\n",
       "      <th>CIK</th>\n",
       "      <th>coDescription_lemmatized</th>\n",
       "      <th>coDescription_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-02-28T00:42:30Z</td>\n",
       "      <td>financialEntities/params;cik=1065280</td>\n",
       "      <td>we are the largest online movie rentalsubscrip...</td>\n",
       "      <td>1065280</td>\n",
       "      <td>we are the largest online movie rentalsubscrip...</td>\n",
       "      <td>largest online movie rentalsubscription servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-02-25T21:22:45Z</td>\n",
       "      <td>financialEntities/params;cik=1065280</td>\n",
       "      <td>with more than 10 millionsubscribers, we are t...</td>\n",
       "      <td>1065280</td>\n",
       "      <td>with more than 10 millionsubscribers , we are ...</td>\n",
       "      <td>millionsubscribers largest online movie rental...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-22T19:09:32Z</td>\n",
       "      <td>financialEntities/params;cik=1065280</td>\n",
       "      <td>with more than12 million subscribers, we are t...</td>\n",
       "      <td>1065280</td>\n",
       "      <td>with more than12 million subscriber , we are t...</td>\n",
       "      <td>million subscriber world largest subscription ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-18T19:42:40Z</td>\n",
       "      <td>financialEntities/params;cik=1065280</td>\n",
       "      <td>us with 20 million subscribers as of december ...</td>\n",
       "      <td>1065280</td>\n",
       "      <td>u with 20 million subscriber a of december 31 ...</td>\n",
       "      <td>u million subscriber december netflix netflix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-02-10T22:29:24Z</td>\n",
       "      <td>financialEntities/params;cik=1065280</td>\n",
       "      <td>us netflix inc. (“netflix”, “the company”, “we...</td>\n",
       "      <td>1065280</td>\n",
       "      <td>u netflix inc. ( “ netflix ” , “ the company ”...</td>\n",
       "      <td>u netflix netflix company u world leading inte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             filingDate                       financialEntity  \\\n",
       "0  2008-02-28T00:42:30Z  financialEntities/params;cik=1065280   \n",
       "1  2009-02-25T21:22:45Z  financialEntities/params;cik=1065280   \n",
       "2  2010-02-22T19:09:32Z  financialEntities/params;cik=1065280   \n",
       "3  2011-02-18T19:42:40Z  financialEntities/params;cik=1065280   \n",
       "4  2012-02-10T22:29:24Z  financialEntities/params;cik=1065280   \n",
       "\n",
       "                                       coDescription      CIK  \\\n",
       "0  we are the largest online movie rentalsubscrip...  1065280   \n",
       "1  with more than 10 millionsubscribers, we are t...  1065280   \n",
       "2  with more than12 million subscribers, we are t...  1065280   \n",
       "3  us with 20 million subscribers as of december ...  1065280   \n",
       "4  us netflix inc. (“netflix”, “the company”, “we...  1065280   \n",
       "\n",
       "                            coDescription_lemmatized  \\\n",
       "0  we are the largest online movie rentalsubscrip...   \n",
       "1  with more than 10 millionsubscribers , we are ...   \n",
       "2  with more than12 million subscriber , we are t...   \n",
       "3  u with 20 million subscriber a of december 31 ...   \n",
       "4  u netflix inc. ( “ netflix ” , “ the company ”...   \n",
       "\n",
       "                             coDescription_stopwords  \n",
       "0  largest online movie rentalsubscription servic...  \n",
       "1  millionsubscribers largest online movie rental...  \n",
       "2  million subscriber world largest subscription ...  \n",
       "3  u million subscriber december netflix netflix ...  \n",
       "4  u netflix netflix company u world leading inte...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_five = pd.DataFrame.from_dict(\n",
    "    {1660134:\"OKTA\",1713683:\"Z-Scaler\",1065280:\"NFLX\",51143:\"IBM\",40545:\"GE\"},\n",
    "    orient = \"index\").reset_index().rename(columns={\"index\":\"CIK\",0:\"name\"})\n",
    "\n",
    "data_five_raw = pd.read_csv(\"../data/bq_dynamic_five.csv\")\n",
    "data_five_raw[\"CIK\"] = data_five_raw[\"financialEntity\"].str.split(\"=\", expand = True).iloc[:,1]\n",
    "data_five = std_func.clean(data_five_raw)\n",
    "data_five.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a189077-ac81-40a7-bcf8-b26e3ce3f5b0",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>filingDate</th>\n",
       "      <th>reportingDate</th>\n",
       "      <th>financialEntity</th>\n",
       "      <th>CIK</th>\n",
       "      <th>coDescription_stopwords</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-29 20:05:22 UTC</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>financialEntities/params;cik=1164888</td>\n",
       "      <td>1164888</td>\n",
       "      <td>disclosure part iiitem marketfor registrant co...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-03 17:53:54 UTC</td>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>financialEntities/params;cik=1637459</td>\n",
       "      <td>1637459</td>\n",
       "      <td>l kraft heinz one largest food beverage compan...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-05-25 01:55:42 UTC</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>financialEntities/params;cik=1302215</td>\n",
       "      <td>1302215</td>\n",
       "      <td>established houlihan lokey leading global inde...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-12-19 22:01:54 UTC</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>financialEntities/params;cik=725363</td>\n",
       "      <td>725363</td>\n",
       "      <td>solved seccomments none office space boone blv...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-02-28 18:47:50 UTC</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>financialEntities/params;cik=103379</td>\n",
       "      <td>103379</td>\n",
       "      <td>orporation organized global leader design prod...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1               filingDate reportingDate  \\\n",
       "0   0  2017-06-29 20:05:22 UTC    2017-04-01   \n",
       "1   1  2016-03-03 17:53:54 UTC    2016-02-01   \n",
       "2   2  2018-05-25 01:55:42 UTC    2018-04-01   \n",
       "3   3  2018-12-19 22:01:54 UTC    2018-10-01   \n",
       "4   4  2018-02-28 18:47:50 UTC    2018-01-01   \n",
       "\n",
       "                        financialEntity      CIK  \\\n",
       "0  financialEntities/params;cik=1164888  1164888   \n",
       "1  financialEntities/params;cik=1637459  1637459   \n",
       "2  financialEntities/params;cik=1302215  1302215   \n",
       "3   financialEntities/params;cik=725363   725363   \n",
       "4   financialEntities/params;cik=103379   103379   \n",
       "\n",
       "                             coDescription_stopwords name  \n",
       "0  disclosure part iiitem marketfor registrant co...  NaN  \n",
       "1  l kraft heinz one largest food beverage compan...  NaN  \n",
       "2  established houlihan lokey leading global inde...  NaN  \n",
       "3  solved seccomments none office space boone blv...  NaN  \n",
       "4  orporation organized global leader design prod...  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The companies with names have them, the ones that don't are NaN\n",
    "clean = pd.merge(\n",
    "    pd.merge(data_t,data.loc[:,[\"CIK\",\"name\"]], how = \"left\", on = \"CIK\"),\n",
    "    the_five, how = \"left\", on=\"CIK\")\n",
    "\n",
    "clean['name'] = clean['name_y'].fillna(clean['name_x'])\n",
    "clean = clean.drop([\"name_x\",\"name_y\",\"reports\"],axis = 1)\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72e559d-309a-4c41-9d8c-ac05c9234650",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>X1</th>\n",
       "      <th>filingDate</th>\n",
       "      <th>reportingDate</th>\n",
       "      <th>financialEntity</th>\n",
       "      <th>CIK</th>\n",
       "      <th>coDescription_stopwords</th>\n",
       "      <th>name</th>\n",
       "      <th>coDescription</th>\n",
       "      <th>coDescription_lemmatized</th>\n",
       "      <th>reports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>836</td>\n",
       "      <td>823.0</td>\n",
       "      <td>2016-11-21 18:23:28 UTC</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>financialEntities/params;cik=2969</td>\n",
       "      <td>2969</td>\n",
       "      <td>al description business air product chemical u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>781</td>\n",
       "      <td>769.0</td>\n",
       "      <td>2018-11-20 19:47:42 UTC</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>financialEntities/params;cik=2969</td>\n",
       "      <td>2969</td>\n",
       "      <td>duct chemical delaware corporation originally ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1972</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>2016-12-06 22:05:12 UTC</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>financialEntities/params;cik=3545</td>\n",
       "      <td>3545</td>\n",
       "      <td>alico alico wa incorporated law state florida ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1474</td>\n",
       "      <td>1456.0</td>\n",
       "      <td>2017-12-11 22:13:06 UTC</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>financialEntities/params;cik=3545</td>\n",
       "      <td>3545</td>\n",
       "      <td>alico alico wa incorporated law state florida ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2018-12-06 21:58:38 UTC</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>financialEntities/params;cik=3545</td>\n",
       "      <td>3545</td>\n",
       "      <td>alico alico wa incorporated law state florida ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      X1               filingDate reportingDate  \\\n",
       "0    836   823.0  2016-11-21 18:23:28 UTC    2016-10-01   \n",
       "1    781   769.0  2018-11-20 19:47:42 UTC    2018-10-01   \n",
       "2   1972  1948.0  2016-12-06 22:05:12 UTC    2016-10-01   \n",
       "3   1474  1456.0  2017-12-11 22:13:06 UTC    2017-10-01   \n",
       "4     86    83.0  2018-12-06 21:58:38 UTC    2018-10-01   \n",
       "\n",
       "                     financialEntity   CIK  \\\n",
       "0  financialEntities/params;cik=2969  2969   \n",
       "1  financialEntities/params;cik=2969  2969   \n",
       "2  financialEntities/params;cik=3545  3545   \n",
       "3  financialEntities/params;cik=3545  3545   \n",
       "4  financialEntities/params;cik=3545  3545   \n",
       "\n",
       "                             coDescription_stopwords name coDescription  \\\n",
       "0  al description business air product chemical u...  NaN           NaN   \n",
       "1  duct chemical delaware corporation originally ...  NaN           NaN   \n",
       "2  alico alico wa incorporated law state florida ...  NaN           NaN   \n",
       "3  alico alico wa incorporated law state florida ...  NaN           NaN   \n",
       "4  alico alico wa incorporated law state florida ...  NaN           NaN   \n",
       "\n",
       "  coDescription_lemmatized  reports  \n",
       "0                      NaN        2  \n",
       "1                      NaN        2  \n",
       "2                      NaN        3  \n",
       "3                      NaN        3  \n",
       "4                      NaN        3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.concat([clean,data_five], axis = 0)\n",
    "final = final.merge(pd.Series(final.groupby(\"CIK\").size(), name = \"reports\"), how = \"left\", on = \"CIK\").sort_values([\"CIK\",\"filingDate\"]).reset_index()\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c6f12-dea9-4b96-9612-43786663e9ec",
   "metadata": {},
   "source": [
    "## Diving into the embeddings\n",
    "\n",
    "### tf-idf (term frequency - inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b736c32a-a0ef-485b-95a4-b6c162134769",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from datetime import datetime\n",
    "\n",
    "def deltas(final, embedding, features):\n",
    "    ignore_words = [\"revenue\",\"fiscal\",\"year\", \"operating\", \"december\", \"ended\", \"administrative\", \"month\", \"company\", \"general\", \"also\",\n",
    "                    \"statement\", \"asset\", \"result\", \"term\", \"september\", \"accounting\", \"million\"]\n",
    "    changes = [[],[],[],[]]\n",
    "    for i in final.loc[:,\"CIK\"]:\n",
    "        # i = final.loc[2,\"CIK\"]\n",
    "        # Get the all company filings\n",
    "        company_filings = embedding[embedding[\"CIK\"] == i].reset_index(drop=True)\n",
    "        # Get the change YoY in tfidf values\n",
    "        delta = pd.DataFrame(np.array(company_filings.iloc[1:,3:]) - np.array(company_filings.iloc[:-1,3:]), columns=features)\n",
    "        # named_delta = pd.concat([company_filings.loc[1:,[\"filingDate\",\"CIK\", \"name\"]].reset_index(drop=True),delta], axis = 1)\n",
    "        # Get the top 20 changed terms in YoY filings\n",
    "        for j in np.arange(company_filings.shape[0]-1):\n",
    "            word_delta = delta.iloc[j,:].sort_values(key=abs, ascending = False).reset_index()\n",
    "            word_delta['flagCol'] = np.where(word_delta.loc[:,\"index\"].str.contains('|'.join(ignore_words)),1,0)\n",
    "            words = word_delta[word_delta['flagCol'] == 0].iloc[:,:2].head(20).reset_index(drop=True).rename(columns = {\"index\":\"topic\",0:\"delta\"})\n",
    "            # year = datetime.strptime(company_filings.loc[j,\"filingDate\"], '%Y-%m-%d %H:%M:%S UTC').date().year\n",
    "            info = pd.concat([pd.Series(i).repeat(20),pd.Series(str(\"year \" + j + \" to year \" + int(j+1))).repeat(20)], axis = 1) \\\n",
    "                .reset_index(drop=True)\\\n",
    "                .rename(columns = {0:\"CIK\",1:\"years\"})\n",
    "            to_append = pd.concat([info,words], axis = 1)\n",
    "            for k in np.arange(to_append.shape[1]):\n",
    "                changes[k].append(to_append.iloc[:,k].tolist())\n",
    "\n",
    "    for i in np.arange(len(changes)):\n",
    "        changes[i] = functools.reduce(operator.iconcat, changes[i], [])\n",
    "        \n",
    "    return(pd.DataFrame(list(zip(changes[0],changes[1],changes[2],changes[3]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51463540-831c-4824-b100-287fdeccc86d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"numpy.int64\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(pipe\u001b[38;5;241m.\u001b[39mtransform(final[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoDescription_stopwords\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m     10\u001b[0m data_tfidf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([final\u001b[38;5;241m.\u001b[39mloc[:,[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIK\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]],tfidf], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m delta_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mdeltas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m delta_tfidf\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mdeltas\u001b[0;34m(final, embedding, features)\u001b[0m\n\u001b[1;32m     20\u001b[0m words \u001b[38;5;241m=\u001b[39m word_delta[word_delta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflagCol\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[:,:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mrename(columns \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# year = datetime.strptime(company_filings.loc[j,\"filingDate\"], '%Y-%m-%d %H:%M:%S UTC').date().year\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mSeries(i)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m20\u001b[39m),pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myear \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to year \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)))\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m20\u001b[39m)], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \\\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mrename(columns \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIK\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myears\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     25\u001b[0m to_append \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([info,words], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(to_append\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"numpy.int64\") to str"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# combine the techniques since tf-idf only augments count vectorized documents\n",
    "pipe = Pipeline([('count', CountVectorizer(ngram_range = (2,4), max_features = 1000)),\n",
    "                  ('tfidf', TfidfTransformer())]).fit(final[\"coDescription_stopwords\"])\n",
    "\n",
    "tfidf = pd.DataFrame(pipe.transform(final[\"coDescription_stopwords\"]).toarray())\n",
    "data_tfidf = pd.concat([final.loc[:,[\"filingDate\",\"CIK\", \"name\"]],tfidf], axis = 1)\n",
    "delta_tfidf = deltas(final, data_tfidf, pipe.get_feature_names_out().tolist())\n",
    "delta_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a654dd6-d30a-494e-ad45-fdcc9ff7851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee81438-5a0e-4eb8-8309-ab571185bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OKTA\n",
    "delta_tfidf[delta_tfidf.iloc[:,0] == 1660134].iloc[:60,:].groupby(1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9cf30-fc3d-4d31-ab99-3547456d7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Scaler\n",
    "delta_tfidf[delta_tfidf.iloc[:,0] == 1713683].iloc[:60,:].groupby(1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9674e1c-9072-4d94-bff5-47ee670ccaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final[final[\"CIK\"] == 1065280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b280fca-9229-42ef-b82d-3db5ed7e69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netflix\n",
    "delta_tfidf[delta_tfidf.iloc[:,0] == 1065280].iloc[:60,:].groupby(1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ba57b-1b3a-431f-ae59-8f5e72270468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GE\n",
    "delta_tfidf[delta_tfidf.iloc[:,0] == 40545].iloc[:60,:].groupby(1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0617cf8-7d17-4362-9aab-97b4b644d63e",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4af095-1042-4b1b-97aa-5a094b5a54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /Users/richardye/Documents/Python/venv_ubineer/bin/activate\n",
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cf58f-af19-49b3-8745-1163801cac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim import utils\n",
    "\n",
    "revs_processed = final[\"coDescription_stopwords\"].apply(lambda x: utils.simple_preprocess(x))\n",
    "\n",
    "# https://stackoverflow.com/questions/46560861/relation-between-word2vec-vector-size-and-total-number-of-words-scanned\n",
    "model_w = Word2Vec(revs_processed, vector_size = 300)\n",
    "\n",
    "def doc_to_vec(text):\n",
    "    word_vecs = [model_w.wv[w] for w in text if w in model_w.wv]\n",
    "    \n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(model_w.vector_size)\n",
    "    \n",
    "    return np.mean(word_vecs, axis = 0)\n",
    "\n",
    "doc_vec = pd.DataFrame(revs_processed.apply(doc_to_vec).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae1371-1829-42fd-8755-a7dbe2fe80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414de31-b562-431f-9414-2c1af1f8b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_yr = pd.concat([final.loc[:,[\"filingDate\",\"CIK\", \"name\"]],doc_vec],axis = 1).groupby(\"CIK\").head(1)\n",
    "print(first_yr.shape)\n",
    "first_yr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c35868-b147-40c4-a5f2-fc5bb2a16786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=.05, min_samples=5, metric='cosine').fit(first_yr.iloc[:,3:])\n",
    "# clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b4c3e-dcd1-4b1b-9fd1-8097567548e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.labels_.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b34aa6-cd3e-4299-85bb-67e670f74706",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(clustering.labels_, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb9a164-d327-466c-b203-aebfe2fa62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_w2v = pd.concat([first_yr.iloc[:,3:],pd.Series(clustering.labels_, name = \"cluster\")], axis = 1) \\\n",
    "    .groupby(\"cluster\").mean()\n",
    "centers_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456df14-42e1-4783-8962-0b0e846dbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# get distances to each cluster center created based on first year filings\n",
    "# want to find representative companies for each cluster\n",
    "cosine_dist = pd.concat([first_yr.loc[:,[\"filingDate\",\"CIK\", \"name\"]].reset_index(drop=True),\n",
    "                         pd.DataFrame(cosine_similarity(first_yr.iloc[:,3:],centers_w2v))],axis = 1)\n",
    "cosine_dist\n",
    "# pd.concat([first_yr.loc[:,[\"filingDate\",\"CIK\", \"name\"]],\n",
    "#            pd.DataFrame(cosine_similarity(first_yr.iloc[:,3:],centers_w2v))], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ebc83-ee6b-4426-9344-5b148a7f7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_companies = []\n",
    "for i in np.arange(3,cosine_dist.shape[1]):\n",
    "    rank = cosine_dist.iloc[:,i].sort_values(ascending = False).index.tolist()[:5]\n",
    "    CIK = first_yr.iloc[rank,1]\n",
    "    rep_words = final[final[\"CIK\"].isin(CIK)].groupby(\"CIK\").head(1).loc[:,\"coDescription\"]\n",
    "    cluster_companies.append(rep_words)\n",
    "cluster_companies[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08288a-db72-4db1-9724-7b69dd3f1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_w2v = pd.concat([final.loc[:,[\"filingDate\",\"CIK\", \"name\"]],doc_vec],axis = 1)\n",
    "yr_2_dist = all_w2v[all_w2v.groupby(\"CIK\").cumcount() == 1]\n",
    "yr_3_dist = all_w2v[all_w2v.groupby(\"CIK\").cumcount() == 2]\n",
    "\n",
    "cosine_dist_2 = pd.concat([yr_2_dist.loc[:,[\"filingDate\",\"CIK\", \"name\"]].reset_index(drop=True),\n",
    "                           pd.DataFrame(cosine_similarity(yr_2_dist.iloc[:,3:],centers_w2v))],axis = 1)\n",
    "cosine_dist_3 = pd.concat([yr_3_dist.loc[:,[\"filingDate\",\"CIK\", \"name\"]].reset_index(drop=True),\n",
    "                           pd.DataFrame(cosine_similarity(yr_3_dist.iloc[:,3:],centers_w2v))],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3088c0-6a02-46ce-8be3-daf1693d0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_y2 = pd.DataFrame(np.array(cosine_dist_2[cosine_dist_2.loc[:,\"CIK\"].isin(cosine_dist.loc[:,\"CIK\"])].iloc[:,3:]) - \\\n",
    "    np.array(cosine_dist[cosine_dist.loc[:,\"CIK\"].isin(cosine_dist_2.loc[:,\"CIK\"])].iloc[:,3:]),\n",
    "                     index = pd.MultiIndex.from_frame(pd.DataFrame(cosine_dist.loc[cosine_dist.loc[:,\"CIK\"].isin(cosine_dist_2.loc[:,\"CIK\"]), \"CIK\"]), names=[\"CIK\"])).reset_index()\n",
    "y1_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389e238-806f-435f-b614-0cb52d5fc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2cb0d-b0c4-4a42-bb44-0fc214687bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_y2[y1_y2[\"CIK\"].isin(the_five[\"CIK\"])].sort_values(by=36, axis =1, key = abs, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9005303-9e51-4551-91e0-6359abfa2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "[w[:300] for w in cluster_companies[2].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78fa756-2e39-47f8-8e6e-6af4e9a297ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_y3 = pd.DataFrame(np.array(cosine_dist_3[cosine_dist_3.loc[:,\"CIK\"].isin(cosine_dist_2.loc[:,\"CIK\"])].iloc[:,3:]) - \\\n",
    "    np.array(cosine_dist_2[cosine_dist_2.loc[:,\"CIK\"].isin(cosine_dist_3.loc[:,\"CIK\"])].iloc[:,3:]),\n",
    "                     index = pd.MultiIndex.from_frame(pd.DataFrame(cosine_dist_2.loc[cosine_dist_2.loc[:,\"CIK\"].isin(cosine_dist_3.loc[:,\"CIK\"]), \"CIK\"]), names=[\"CIK\"])).reset_index()\n",
    "y2_y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026f6b6-8845-4635-b01d-25afb58e048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_y3[y2_y3[\"CIK\"].isin(the_five[\"CIK\"])].sort_values(by=32, axis =1, key = abs, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c2b30-544f-40be-a421-f89bc4c8b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_func.pca_visualize_2d(doc_vec, pd.DataFrame(clustering.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe6bec-a279-4f0a-8883-d2abb6caef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w.wv.most_similar(positive =['ibm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf9aa2-e7d4-48bd-a063-6e3317e0b459",
   "metadata": {},
   "source": [
    "The numer of features is actually the number of dimensions, so there are 300 \"topics\" in this space currently. We'll reduce that before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25508ece-3c3a-4072-b215-d043bdc82cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since its not sparse, PCA should work just fine\n",
    "multi_index = pd.MultiIndex.from_frame(final.loc[:,[\"filingDate\",\"CIK\", \"name\"]])\n",
    "    \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10)\n",
    "pca_embedding = pca.fit_transform(doc_vec)\n",
    "pca_embedding = pd.DataFrame(pca_embedding, index = multi_index).reset_index()\n",
    "pca_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e571f-73da-45e6-af1e-8a84715a21a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word2vec = pd.concat([final.loc[:,[\"filingDate\",\"CIK\", \"name\"]],doc_vec], axis = 1)\n",
    "delta_word2vec = deltas(final, data_word2vec, model_w.wv.index_to_key)\n",
    "delta_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef5485-d03c-4b7a-932c-1ada89c9f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bb23a-0260-4ee0-9505-3a52e4886194",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c9c81-d0ca-4a3c-999e-3a140ebc8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(final[\"coDescription_stopwords\"]):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "\n",
    "model = doc2vec.Doc2Vec(docs, vector_size = 100, window = 10, min_count = 1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9defd-1d88-48ad-a16e-018a0cb0852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec_2 = pd.DataFrame([model.dv[doc] for doc in np.arange(0,len(docs))])\n",
    "doc_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aae3fb-6759-4ad8-8d97-1e5fa3fee2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN()\n",
    "clusterer.fit(doc_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812f5b7-96ea-4a23-b19f-f81e9cafaaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.labels_.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

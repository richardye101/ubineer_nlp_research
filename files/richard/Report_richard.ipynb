{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Research into public 10K filings\n",
    "\n",
    "In this project, Mary Xu, Peitong Lu, and Richard Ye worked with the `business description` of 1500+ 10K Annual Report Filings from SEC, with support from [Ubineer](https://www.ubineer.com/).\n",
    "\n",
    "# Packages used\n",
    "\n",
    "- os\n",
    "- json\n",
    "- re\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- nltk\n",
    "- bertopic\n",
    "- matplotlib\n",
    "- plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics explored\n",
    "\n",
    "1. Word Embedding techniques\n",
    "2. Distance metrics on those word embeddings\n",
    "3. Topic Modelling/Emebedding\n",
    "4. Dynamic Word Embedding comparisons\n",
    "\n",
    "## Data cleaning/processing\n",
    "\n",
    "The data was provided to us by Ubineer, which has been pulled and preprocessed for us. One of the main datasets we used is the `bq_2018_top5SIC.json` file prepared by Professor Sotiros Damouras, by selecting companies who have filed in 2018 and belong to the top 5 industries within the dataset. This file has 1127 filings (one per company).\n",
    "\n",
    "The file schema contains the columns:\n",
    "- `accessionNumber`\n",
    "- `filingDate`\n",
    "- `reportingDate`\n",
    "- `financialEntity`\n",
    "- `htmlFile`\n",
    "- `coDescription`\n",
    "- `CIK`\n",
    "- `name`\n",
    "- `countryinc`\n",
    "- `cityma`\n",
    "- `SIC`\n",
    "- `SIC_desc`\n",
    "\n",
    "For our purposes, we will be focusing on `name` (identifies the company), `coDescription` (the Business Overview), `SIC_desc` (the industry they operate in)\n",
    "\n",
    "Within our pre-processing, we focus on `coDescription`. \n",
    "We further cleaned up the `Description` text by removing HTML code, the first couple words which were common among all filings such as _\"business overview\"_, and filtering for filings with over 250 characters.\n",
    "\n",
    "We then removed _stop words_ from the business descriptions, which are very commong words like \"the, they, and, is, are, there\" and others. These words don't provide meaning and therefore do not contribute to our goal of extracting meaning.\n",
    "\n",
    "We also lemmatized all possible words, aka Text/Word Normalization which means all instances of \"am, are, is\" are converted to \"be\" and \"playing, played, plays\" are all converted to \"play\". This reduces the amount of different words we have to process, and also condensing the amount of information we recieve since words that all carry the same meaning are represented together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"bq_2018_top5SIC.json\", lines = True)\n",
    "\n",
    "#strip any left over html code\n",
    "def clean_data_fn(insrt_data):\n",
    "    clean_data = []\n",
    "    for idx, ele in insrt_data.iterrows():\n",
    "        if \"https://www.sec.gov/Archives/edgar/data/\" in ele[\"coDescription\"]:\n",
    "            pass\n",
    "        else:\n",
    "            clean_txt = re.compile('<.*?>')\n",
    "            desc = re.sub(clean_txt,'',ele[\"coDescription\"]).replace(u'\\xa0', u' ').replace(\"   \", \"\").replace(\"'\", \"\").replace('\"','')\n",
    "            if re.search('<', desc):\n",
    "                pos = re.search('<', desc).start()\n",
    "            desc = desc[:pos].lower()\n",
    "            if (desc.find(\"business\") >= 20): # didnt find it in the first 20 characters then look for next\n",
    "                desc = desc[6 : ( desc.rfind(\"<\") )] # remove the \"Item 1.\" stuff only\n",
    "            else: # found \"business\", remove everything before it\n",
    "                desc =  desc[( desc.find(\"business\") + 8 ) : ( desc.rfind(\"<\") ) ]\n",
    "            if (desc.find(\"overview\") <= 20): # didnt find it in the first 20 characters then look for next\n",
    "                desc =  desc[( desc.find(\"overview\") + 8 ) :]\n",
    "            # remove leading white space and periods\n",
    "            desc = re.sub(r\"^\\.\", \"\", desc).strip()            \n",
    "            new_data = ele.copy()\n",
    "            new_data[\"coDescription\"] = desc\n",
    "            # remove any filings with a description less than 250 characters (not enough information for us)\n",
    "            if len(desc)<250:\n",
    "                pass\n",
    "            else:\n",
    "                clean_data.append(new_data)\n",
    "                \n",
    "    return(pd.DataFrame(clean_data))\n",
    "\n",
    "non_html_data = clean_data_fn(df)#.rename(columns = {\"financialEntity\":\"CIK\"})\n",
    "non_html_data[\"CIK\"] = non_html_data[\"CIK\"].astype(int)\n",
    "\n",
    "#lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "def lemmatize_sentence(sentence):\n",
    "    lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)]\n",
    "    return \" \".join(lemmatized_output)\n",
    "\n",
    "lemma_desc = non_html_data[\"coDescription\"].apply(lemmatize_sentence)\n",
    "non_html_data[\"coDescription_lemmatized\"] = lemma_desc\n",
    "non_html_data[\"coDescription_lemmatized\"].head()\n",
    "\n",
    "# remove all numbers so they don't show up as dimensions\n",
    "def remove_nums(x):\n",
    "    text = x.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# remove stopwords and punctuation\n",
    "def remove_stopwords(x):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    word_tokens = word_tokenize(x)\n",
    "\n",
    "    filtered_sentence = ' '.join([w for w in word_tokens if not w.lower() in stop_words and w.isalnum()])\n",
    "\n",
    "    return(filtered_sentence)\n",
    "\n",
    "rm_num_stopwords = non_html_data[\"coDescription_lemmatized\"].apply(remove_nums).apply(remove_stopwords)\n",
    "non_html_data[\"coDescription_stopwords\"] = rm_num_stopwords\n",
    "\n",
    "non_html_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and visualization techniques\n",
    "\n",
    "In this project we focused on visually examining 2 and 3 dimensional plots of the word embeddings reduced using PCA and Truncated SVD (specifically for LSA). We also had access to extra information for 2018 filings, allowing us to evaluate the word embedding clusters against their actual industry classification. This was done using a simple 1-NN clustering. The results were put into a confusion matrix, allowing us to identify how well the 1-NN clutsering did on our word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Word Embeddings techniques\n",
    "\n",
    "## Term Frequency/Counter Vectorizer/Bag of Words\n",
    "\n",
    "We started off with the basic Term Frequency Matrix, which breaks down each company description into a vector of `n` words/terms (a hyperparameter), where each dimension is a word/term, and the value is the count of that word in the document. Obviously the number of unique words in a document can be very large, but in this technique, we select only the `n` words that occur the most in all documents.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\text{The formula:}\\ \\ \\text{tf}(t,d) &= |t| \\text{ in document}\\\\\n",
    "\\end{align}\n",
    "\n",
    "This technique helps us analyze how many of the `n` words each filing contains, which provides us with information about the kind of terms or topics each company may discuss. This approach is very easy to implement, but is not very powerful because very common words will have the largest values and therefore carry the most weight. For financial statements like these, you can expect words like \"financial\" and \"report\" to have some of the highest values.\n",
    "\n",
    "From these vectors for each company filing, we can think of each term as a dimension and actually project these `n` dimensional vectors into an `n` dimensional space, which is called a **word embedding**. You can think of these as points in a `n`D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (tf-idf)\n",
    "\n",
    "To solve the above issue, we moved on to the tf-idf technique. tf-idf augments the term frequency matrix we created above by multiplying each word in each docuemnt by its \"importance\" to that document. The details are within the [1_Tf-idf_analysis.ipynb](https://github.com/richardye101/ubineer_nlp_research/blob/main/content/richard/1_Tf-idf_analysis.ipynb) notebook. This technique is meant to adjust the weighting of terms used in the word embedding so that the _points_ used to represent each company filing is more accurate in representing where companies are in this `n` dimensional space in comparison to other companies. For example, ideally we want technology companies close together, and pharmaceutical companies close together.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{tf-idf}(t,d) &= \\text{tf}(t,d) \\cdot \\text{idf}(t,d)\\\\ \\\\\n",
    "\\text{Where: } \\quad \\text{tf}(t,d) &= |t| \\text{ in document}\\\\ \\\\\n",
    "\\text{idf}(t,d) &= \\log\\frac{N}{\\text{df}(t)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

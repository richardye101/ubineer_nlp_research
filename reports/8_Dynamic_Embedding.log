Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/jupyter_cache/executors/utils.py", line 51, in single_nb_execution
    executenb(
  File "/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nbclient/client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/asyncio/base_events.py", line 616, in run_until_complete
    return future.result()
  File "/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nbclient/client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nbclient/client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nbclient/client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
the_five = pd.DataFrame.from_dict(
    {1660134:"OKTA",1713683:"Z-Scaler",1065280:"NFLX",51143:"IBM",40545:"GE"},
    orient = "index").reset_index().rename(columns={"index":"CIK",0:"name"})

data_five_raw = pd.read_csv("../data/bq_dynamic_five.csv")
data_five_raw["CIK"] = data_five_raw["financialEntity"].str.split("=", expand = True).iloc[:,1]
data_five = std_func.clean(data_five_raw)
data_five.head()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mLookupError[0m                               Traceback (most recent call last)
Input [0;32mIn [2][0m, in [0;36m<cell line: 7>[0;34m()[0m
[1;32m      5[0m data_five_raw [38;5;241m=[39m pd[38;5;241m.[39mread_csv([38;5;124m"[39m[38;5;124m../data/bq_dynamic_five.csv[39m[38;5;124m"[39m)
[1;32m      6[0m data_five_raw[[38;5;124m"[39m[38;5;124mCIK[39m[38;5;124m"[39m] [38;5;241m=[39m data_five_raw[[38;5;124m"[39m[38;5;124mfinancialEntity[39m[38;5;124m"[39m][38;5;241m.[39mstr[38;5;241m.[39msplit([38;5;124m"[39m[38;5;124m=[39m[38;5;124m"[39m, expand [38;5;241m=[39m [38;5;28;01mTrue[39;00m)[38;5;241m.[39miloc[:,[38;5;241m1[39m]
[0;32m----> 7[0m data_five [38;5;241m=[39m [43mstd_func[49m[38;5;241;43m.[39;49m[43mclean[49m[43m([49m[43mdata_five_raw[49m[43m)[49m
[1;32m      8[0m data_five[38;5;241m.[39mhead()

File [0;32m~/work/ubineer_nlp_research/ubineer_nlp_research/content/richard/../std_func.py:119[0m, in [0;36mclean[0;34m(df)[0m
[1;32m    116[0m non_html_data [38;5;241m=[39m clean_data_fn(df)[38;5;66;03m#.rename(columns = {"financialEntity":"CIK"})[39;00m
[1;32m    117[0m non_html_data[[38;5;124m"[39m[38;5;124mCIK[39m[38;5;124m"[39m] [38;5;241m=[39m non_html_data[[38;5;124m"[39m[38;5;124mCIK[39m[38;5;124m"[39m][38;5;241m.[39mastype([38;5;28mint[39m)
[0;32m--> 119[0m lemma_desc [38;5;241m=[39m [43mnon_html_data[49m[43m[[49m[38;5;124;43m"[39;49m[38;5;124;43mcoDescription[39;49m[38;5;124;43m"[39;49m[43m][49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[43mlemmatize_sentence[49m[43m)[49m
[1;32m    120[0m non_html_data[[38;5;124m"[39m[38;5;124mcoDescription_lemmatized[39m[38;5;124m"[39m] [38;5;241m=[39m lemma_desc
[1;32m    121[0m non_html_data[[38;5;124m"[39m[38;5;124mcoDescription_lemmatized[39m[38;5;124m"[39m][38;5;241m.[39mhead()

File [0;32m/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/pandas/core/series.py:4433[0m, in [0;36mSeries.apply[0;34m(self, func, convert_dtype, args, **kwargs)[0m
[1;32m   4323[0m [38;5;28;01mdef[39;00m [38;5;21mapply[39m(
[1;32m   4324[0m     [38;5;28mself[39m,
[1;32m   4325[0m     func: AggFuncType,
[0;32m   (...)[0m
[1;32m   4328[0m     [38;5;241m*[39m[38;5;241m*[39mkwargs,
[1;32m   4329[0m ) [38;5;241m-[39m[38;5;241m>[39m DataFrame [38;5;241m|[39m Series:
[1;32m   4330[0m     [38;5;124;03m"""[39;00m
[1;32m   4331[0m [38;5;124;03m    Invoke function on values of Series.[39;00m
[1;32m   4332[0m 
[0;32m   (...)[0m
[1;32m   4431[0m [38;5;124;03m    dtype: float64[39;00m
[1;32m   4432[0m [38;5;124;03m    """[39;00m
[0;32m-> 4433[0m     [38;5;28;01mreturn[39;00m [43mSeriesApply[49m[43m([49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[43mfunc[49m[43m,[49m[43m [49m[43mconvert_dtype[49m[43m,[49m[43m [49m[43margs[49m[43m,[49m[43m [49m[43mkwargs[49m[43m)[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[43m)[49m

File [0;32m/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/pandas/core/apply.py:1082[0m, in [0;36mSeriesApply.apply[0;34m(self)[0m
[1;32m   1078[0m [38;5;28;01mif[39;00m [38;5;28misinstance[39m([38;5;28mself[39m[38;5;241m.[39mf, [38;5;28mstr[39m):
[1;32m   1079[0m     [38;5;66;03m# if we are a string, try to dispatch[39;00m
[1;32m   1080[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39mapply_str()
[0;32m-> 1082[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mapply_standard[49m[43m([49m[43m)[49m

File [0;32m/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/pandas/core/apply.py:1137[0m, in [0;36mSeriesApply.apply_standard[0;34m(self)[0m
[1;32m   1131[0m         values [38;5;241m=[39m obj[38;5;241m.[39mastype([38;5;28mobject[39m)[38;5;241m.[39m_values
[1;32m   1132[0m         [38;5;66;03m# error: Argument 2 to "map_infer" has incompatible type[39;00m
[1;32m   1133[0m         [38;5;66;03m# "Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],[39;00m
[1;32m   1134[0m         [38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],[39;00m
[1;32m   1135[0m         [38;5;66;03m# List[Union[Callable[..., Any], str]]]]]"; expected[39;00m
[1;32m   1136[0m         [38;5;66;03m# "Callable[[Any], Any]"[39;00m
[0;32m-> 1137[0m         mapped [38;5;241m=[39m [43mlib[49m[38;5;241;43m.[39;49m[43mmap_infer[49m[43m([49m
[1;32m   1138[0m [43m            [49m[43mvalues[49m[43m,[49m
[1;32m   1139[0m [43m            [49m[43mf[49m[43m,[49m[43m  [49m[38;5;66;43;03m# type: ignore[arg-type][39;49;00m
[1;32m   1140[0m [43m            [49m[43mconvert[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mconvert_dtype[49m[43m,[49m
[1;32m   1141[0m [43m        [49m[43m)[49m
[1;32m   1143[0m [38;5;28;01mif[39;00m [38;5;28mlen[39m(mapped) [38;5;129;01mand[39;00m [38;5;28misinstance[39m(mapped[[38;5;241m0[39m], ABCSeries):
[1;32m   1144[0m     [38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested[39;00m
[1;32m   1145[0m     [38;5;66;03m#  See also GH#25959 regarding EA support[39;00m
[1;32m   1146[0m     [38;5;28;01mreturn[39;00m obj[38;5;241m.[39m_constructor_expanddim([38;5;28mlist[39m(mapped), index[38;5;241m=[39mobj[38;5;241m.[39mindex)

File [0;32m/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2870[0m, in [0;36mpandas._libs.lib.map_infer[0;34m()[0m

File [0;32m~/work/ubineer_nlp_research/ubineer_nlp_research/content/richard/../std_func.py:96[0m, in [0;36mlemmatize_sentence[0;34m(sentence)[0m
[1;32m     93[0m [38;5;28;01mdef[39;00m [38;5;21mlemmatize_sentence[39m(sentence):
[1;32m     94[0m     [38;5;66;03m#lemmatization[39;00m
[1;32m     95[0m     lemmatizer [38;5;241m=[39m WordNetLemmatizer()
[0;32m---> 96[0m     lemmatized_output [38;5;241m=[39m [lemmatizer[38;5;241m.[39mlemmatize(w) [38;5;28;01mfor[39;00m w [38;5;129;01min[39;00m [43mword_tokenize[49m[43m([49m[43msentence[49m[43m)[49m]
[1;32m     97[0m     [38;5;28;01mreturn[39;00m [38;5;124m"[39m[38;5;124m [39m[38;5;124m"[39m[38;5;241m.[39mjoin(lemmatized_output)

File [0;32m/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nltk/tokenize/__init__.py:129[0m, in [0;36mword_tokenize[0;34m(text, language, preserve_line)[0m
[1;32m    114[0m [38;5;28;01mdef[39;00m [38;5;21mword_tokenize[39m(text, language[38;5;241m=[39m[38;5;124m"[39m[38;5;124menglish[39m[38;5;124m"[39m, preserve_line[38;5;241m=[39m[38;5;28;01mFalse[39;00m):
[1;32m    115[0m     [38;5;124;03m"""[39;00m
[1;32m    116[0m [38;5;124;03m    Return a tokenized copy of *text*,[39;00m
[1;32m    117[0m [38;5;124;03m    using NLTK's recommended word tokenizer[39;00m
[0;32m   (...)[0m
[1;32m    127[0m [38;5;124;03m    :type preserve_line: bool[39;00m
[1;32m    128[0m [38;5;124;03m    """[39;00m
[0;32m--> 129[0m     sentences [38;5;241m=[39m [text] [38;5;28;01mif[39;00m preserve_line [38;5;28;01melse[39;00m [43msent_tokenize[49m[43m([49m[43mtext[49m[43m,[49m[43m [49m[43mlanguage[49m[43m)[49m
[1;32m    130[0m     [38;5;28;01mreturn[39;00m [
[1;32m    131[0m         token [38;5;28;01mfor[39;00m sent [38;5;129;01min[39;00m sentences [38;5;28;01mfor[39;00m token [38;5;129;01min[39;00m _treebank_word_tokenizer[38;5;241m.[39mtokenize(sent)
[1;32m    132[0m     ]

File [0;32m/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nltk/tokenize/__init__.py:106[0m, in [0;36msent_tokenize[0;34m(text, language)[0m
[1;32m     96[0m [38;5;28;01mdef[39;00m [38;5;21msent_tokenize[39m(text, language[38;5;241m=[39m[38;5;124m"[39m[38;5;124menglish[39m[38;5;124m"[39m):
[1;32m     97[0m     [38;5;124;03m"""[39;00m
[1;32m     98[0m [38;5;124;03m    Return a sentence-tokenized copy of *text*,[39;00m
[1;32m     99[0m [38;5;124;03m    using NLTK's recommended sentence tokenizer[39;00m
[0;32m   (...)[0m
[1;32m    104[0m [38;5;124;03m    :param language: the model name in the Punkt corpus[39;00m
[1;32m    105[0m [38;5;124;03m    """[39;00m
[0;32m--> 106[0m     tokenizer [38;5;241m=[39m [43mload[49m[43m([49m[38;5;124;43mf[39;49m[38;5;124;43m"[39;49m[38;5;124;43mtokenizers/punkt/[39;49m[38;5;132;43;01m{[39;49;00m[43mlanguage[49m[38;5;132;43;01m}[39;49;00m[38;5;124;43m.pickle[39;49m[38;5;124;43m"[39;49m[43m)[49m
[1;32m    107[0m     [38;5;28;01mreturn[39;00m tokenizer[38;5;241m.[39mtokenize(text)

File [0;32m/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nltk/data.py:750[0m, in [0;36mload[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)[0m
[1;32m    747[0m     [38;5;28mprint[39m([38;5;124mf[39m[38;5;124m"[39m[38;5;124m<<Loading [39m[38;5;132;01m{[39;00mresource_url[38;5;132;01m}[39;00m[38;5;124m>>[39m[38;5;124m"[39m)
[1;32m    749[0m [38;5;66;03m# Load the resource.[39;00m
[0;32m--> 750[0m opened_resource [38;5;241m=[39m [43m_open[49m[43m([49m[43mresource_url[49m[43m)[49m
[1;32m    752[0m [38;5;28;01mif[39;00m [38;5;28mformat[39m [38;5;241m==[39m [38;5;124m"[39m[38;5;124mraw[39m[38;5;124m"[39m:
[1;32m    753[0m     resource_val [38;5;241m=[39m opened_resource[38;5;241m.[39mread()

File [0;32m/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nltk/data.py:876[0m, in [0;36m_open[0;34m(resource_url)[0m
[1;32m    873[0m protocol, path_ [38;5;241m=[39m split_resource_url(resource_url)
[1;32m    875[0m [38;5;28;01mif[39;00m protocol [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mor[39;00m protocol[38;5;241m.[39mlower() [38;5;241m==[39m [38;5;124m"[39m[38;5;124mnltk[39m[38;5;124m"[39m:
[0;32m--> 876[0m     [38;5;28;01mreturn[39;00m [43mfind[49m[43m([49m[43mpath_[49m[43m,[49m[43m [49m[43mpath[49m[43m [49m[38;5;241;43m+[39;49m[43m [49m[43m[[49m[38;5;124;43m"[39;49m[38;5;124;43m"[39;49m[43m][49m[43m)[49m[38;5;241m.[39mopen()
[1;32m    877[0m [38;5;28;01melif[39;00m protocol[38;5;241m.[39mlower() [38;5;241m==[39m [38;5;124m"[39m[38;5;124mfile[39m[38;5;124m"[39m:
[1;32m    878[0m     [38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:[39;00m
[1;32m    879[0m     [38;5;28;01mreturn[39;00m find(path_, [[38;5;124m"[39m[38;5;124m"[39m])[38;5;241m.[39mopen()

File [0;32m/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nltk/data.py:583[0m, in [0;36mfind[0;34m(resource_name, paths)[0m
[1;32m    581[0m sep [38;5;241m=[39m [38;5;124m"[39m[38;5;124m*[39m[38;5;124m"[39m [38;5;241m*[39m [38;5;241m70[39m
[1;32m    582[0m resource_not_found [38;5;241m=[39m [38;5;124mf[39m[38;5;124m"[39m[38;5;130;01m\n[39;00m[38;5;132;01m{[39;00msep[38;5;132;01m}[39;00m[38;5;130;01m\n[39;00m[38;5;132;01m{[39;00mmsg[38;5;132;01m}[39;00m[38;5;130;01m\n[39;00m[38;5;132;01m{[39;00msep[38;5;132;01m}[39;00m[38;5;130;01m\n[39;00m[38;5;124m"[39m
[0;32m--> 583[0m [38;5;28;01mraise[39;00m [38;5;167;01mLookupError[39;00m(resource_not_found)

[0;31mLookupError[0m: 
**********************************************************************
  Resource [93mpunkt[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('punkt')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m

  Searched in:
    - '/home/runner/nltk_data'
    - '/opt/hostedtoolcache/Python/3.8.12/x64/nltk_data'
    - '/opt/hostedtoolcache/Python/3.8.12/x64/share/nltk_data'
    - '/opt/hostedtoolcache/Python/3.8.12/x64/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************

LookupError: 
**********************************************************************
  Resource [93mpunkt[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('punkt')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtokenizers/punkt/PY3/english.pickle[0m

  Searched in:
    - '/home/runner/nltk_data'
    - '/opt/hostedtoolcache/Python/3.8.12/x64/nltk_data'
    - '/opt/hostedtoolcache/Python/3.8.12/x64/share/nltk_data'
    - '/opt/hostedtoolcache/Python/3.8.12/x64/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************


